{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main KernelPyLogit Example\n",
    "\n",
    "The purpose of this Jupyter notebook is to demonstrate the key functionalities of PyKernelLogit by means of a real case study. Take into account that some of the functionalities described here are inherited from the original PyLogit package, especially those related with the estimation of conditional logit models. This example is divided in several parts.\n",
    "1. Loading and preprocessing the data. This involves converting a dataframe between 'wide' and 'long' format.\n",
    "2. Estimating a linear Random Utility model using a conditional logit model.\n",
    "    1. Definition of a conditional logit model.\n",
    "    2. Estimation of the previous model.\n",
    "    3. Evaluation of the estimated model.\n",
    "    4. Prediction using the estimated model.\n",
    "    5. Computation of the Willingness to Pay (WTP) indicator using the estimated model.\n",
    "3. Estimating a non-parametric model using Kernel Logistic Regression (KLR).\n",
    "    1. Definition of a KLR model.\n",
    "    2. Estimation of the previous KLR model.\n",
    "    3. Evaluation of the estimated model.\n",
    "    4. Prediction using the estimated model.\n",
    "    5. Computation of the Willingness to Pay (WTP) indicator using estimated KLR model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset selected for this example is the 'Mode' dataset from the 'mlogit' package for R programming language (https://cran.r-project.org/web/packages/mlogit/mlogit.pdf). This dataset was elaborated by a project team in 'Discrete choice methods with simulation' (Train, 2003) and contains data about the mode choice of $453$ commuters. Each commuter has four available travel modes for their trips to work in a choice set $C=\\{ \\hbox{}bus, car, carpool, rail  \\}$. The time and cost of travel for each mode were determined for each commuter. The attributes of the alternative $i \\in C$ for the commuter $n$  are $x_{in}=(x_{in,cost}, x_{in,time})$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the different packages that will be used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict    # For recording the model specification \n",
    "\n",
    "import pandas as pd                    # For file input/output\n",
    "import numpy as np                     # For vectorized math operations\n",
    "\n",
    "import PyKernelLogit as pkl            # For the estimation of the MNL and KLR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the data\n",
    "\n",
    "### 1.1. Load the data\n",
    "The first step to estimate a model using PyKernelLogit is loading the dataset using a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>choice</td>\n",
       "      <td>car</td>\n",
       "      <td>rail</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cost.car</td>\n",
       "      <td>1.50701</td>\n",
       "      <td>6.057</td>\n",
       "      <td>5.79468</td>\n",
       "      <td>1.86914</td>\n",
       "      <td>2.49895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cost.carpool</td>\n",
       "      <td>2.33561</td>\n",
       "      <td>2.89692</td>\n",
       "      <td>2.13745</td>\n",
       "      <td>2.57243</td>\n",
       "      <td>1.72201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cost.bus</td>\n",
       "      <td>1.80051</td>\n",
       "      <td>2.23713</td>\n",
       "      <td>2.57638</td>\n",
       "      <td>1.90352</td>\n",
       "      <td>2.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cost.rail</td>\n",
       "      <td>2.35892</td>\n",
       "      <td>1.85545</td>\n",
       "      <td>2.74748</td>\n",
       "      <td>2.26828</td>\n",
       "      <td>2.97387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.car</td>\n",
       "      <td>18.5032</td>\n",
       "      <td>31.3111</td>\n",
       "      <td>22.5474</td>\n",
       "      <td>26.0903</td>\n",
       "      <td>4.69914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.carpool</td>\n",
       "      <td>26.3382</td>\n",
       "      <td>34.257</td>\n",
       "      <td>23.2552</td>\n",
       "      <td>29.896</td>\n",
       "      <td>12.4141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.bus</td>\n",
       "      <td>20.8678</td>\n",
       "      <td>67.1819</td>\n",
       "      <td>63.3091</td>\n",
       "      <td>19.7527</td>\n",
       "      <td>43.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time.rail</td>\n",
       "      <td>30.0335</td>\n",
       "      <td>60.2931</td>\n",
       "      <td>49.1716</td>\n",
       "      <td>13.4727</td>\n",
       "      <td>39.7433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0        1        2        3        4\n",
       "choice            car     rail      car      car      car\n",
       "cost.car      1.50701    6.057  5.79468  1.86914  2.49895\n",
       "cost.carpool  2.33561  2.89692  2.13745  2.57243  1.72201\n",
       "cost.bus      1.80051  2.23713  2.57638  1.90352    2.686\n",
       "cost.rail     2.35892  1.85545  2.74748  2.26828  2.97387\n",
       "time.car      18.5032  31.3111  22.5474  26.0903  4.69914\n",
       "time.carpool  26.3382   34.257  23.2552   29.896  12.4141\n",
       "time.bus      20.8678  67.1819  63.3091  19.7527   43.092\n",
       "time.rail     30.0335  60.2931  49.1716  13.4727  39.7433"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Mode dataset from a .csv file. Nota that all the elements are delimited by a ',' character.\n",
    "mode_choice = pd.read_csv(\"../data/ModeChoice.csv\", sep=',')\n",
    "\n",
    "# Show the first 5 rows of the data\n",
    "mode_choice.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Convert the dataset to long format\n",
    "\n",
    "In order to estimate models using the PyKernelLogit library it is necessary to convert the data to \"long\" format.\n",
    "\n",
    "Long format has 1 row per individual per available alternative, and wide format has 1 row per individual or observation. Long format is useful because it permits one to directly use matrix dot products to calculate the index, $V_{ij} = x_{ij} \\beta$, for each individual $\\left(i \\right)$ for each alternative $\\left(j \\right)$. In situations where a dataset is provided in wide format (as in the case of the Mode dataset), it will be necessary to convert the data from wide format to long format.\n",
    "\n",
    "To convert a dataset to long format, it is needed to specify:\n",
    "<ol>\n",
    "    <li>The variables or columns that are specific to a given individual, regardless of what alternative is being considered.</li>\n",
    "    <li>The variables that vary across some or all alternatives, for a given individual (e.g. travel time).</li>\n",
    "    <li>The availability variables.</li>\n",
    "    <li>The <u>unique</u> observation id column.</li>\n",
    "    <li>The choice column.</li>\n",
    "</ol>\n",
    "\n",
    "The following cells of code identify perform the necessary conversions from the wide to the long dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['choice', 'cost.car', 'cost.carpool', 'cost.bus', 'cost.rail',\n",
       "       'time.car', 'time.carpool', 'time.bus', 'time.rail'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the columns of the Mode dataset\n",
    "mode_choice.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert choice from categorical variable to numeric one\n",
    "mode_choice['choice'] = mode_choice['choice'].map({'car' : 1, 'carpool' : 2 , 'bus' : 3, 'rail' : 4})\n",
    "\n",
    "# Create the list of individual specific variables\n",
    "ind_variables = [] # There are no indivicual specific variables in this dataset\n",
    "\n",
    "# Variables that vary across some or all alternatives, for a given individual\n",
    "alt_varying_variables = {u'cost': dict([(1, 'cost.car'),\n",
    "                                        (2, 'cost.carpool'),\n",
    "                                        (3, 'cost.bus'),\n",
    "                                        (4, 'cost.rail')]),\n",
    "                         u'time': dict([(1, 'time.car'),\n",
    "                                        (2, 'time.carpool'),\n",
    "                                        (3, 'time.bus'),\n",
    "                                        (4, 'time.rail')])\n",
    "                        }\n",
    "\n",
    "### Specify the availability variables\n",
    "# Note that the keys of the dictionary are the alternative id's.\n",
    "# The values are the columns denoting the availability for the\n",
    "# given mode in the dataset. If the dataset does not contains any\n",
    "# availability columns (such as in the Mode dataset), then the \n",
    "# keyword 'available_for_all' can be used to specify that an alternative\n",
    "# is availables for all individuals.\n",
    "availability_variables = {1: 'available_for_all',\n",
    "                          2: 'available_for_all', \n",
    "                          3: 'available_for_all',\n",
    "                          4: 'available_for_all'}\n",
    "\n",
    "### Determine the columns for: alternative ids, the observation ids and the choice\n",
    "\n",
    "# The 'custom_alt_id' is the name of a column to be created in the long format data\n",
    "# It will identify the alternative associated with each row.\n",
    "custom_alt_id = \"mode_id\"\n",
    "\n",
    "# Create a custom id column. Note the +1 ensures the id's start at one.\n",
    "obs_id_column = \"custom_id\"\n",
    "mode_choice[obs_id_column] = np.arange(mode_choice.shape[0], dtype=int) + 1\n",
    "\n",
    "# Create a variable recording the choice column\n",
    "choice_column = \"choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the conversion to long format\n",
    "long_mode_choice = pkl.convert_wide_to_long(mode_choice, \n",
    "                                           ind_variables, \n",
    "                                           alt_varying_variables, \n",
    "                                           availability_variables, \n",
    "                                           obs_id_column, \n",
    "                                           choice_column,\n",
    "                                           new_alt_id_name=custom_alt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>custom_id</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mode_id</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>choice</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cost</td>\n",
       "      <td>1.50701</td>\n",
       "      <td>2.335612</td>\n",
       "      <td>1.800512</td>\n",
       "      <td>2.358920</td>\n",
       "      <td>6.056998</td>\n",
       "      <td>2.896919</td>\n",
       "      <td>2.237128</td>\n",
       "      <td>1.855451</td>\n",
       "      <td>5.794677</td>\n",
       "      <td>2.137454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>time</td>\n",
       "      <td>18.50320</td>\n",
       "      <td>26.338233</td>\n",
       "      <td>20.867794</td>\n",
       "      <td>30.033469</td>\n",
       "      <td>31.311107</td>\n",
       "      <td>34.256956</td>\n",
       "      <td>67.181889</td>\n",
       "      <td>60.293126</td>\n",
       "      <td>22.547429</td>\n",
       "      <td>23.255171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1          2          3          4          5  \\\n",
       "custom_id   1.00000   1.000000   1.000000   1.000000   2.000000   2.000000   \n",
       "mode_id     1.00000   2.000000   3.000000   4.000000   1.000000   2.000000   \n",
       "choice      1.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "cost        1.50701   2.335612   1.800512   2.358920   6.056998   2.896919   \n",
       "time       18.50320  26.338233  20.867794  30.033469  31.311107  34.256956   \n",
       "\n",
       "                   6          7          8          9  \n",
       "custom_id   2.000000   2.000000   3.000000   3.000000  \n",
       "mode_id     3.000000   4.000000   1.000000   2.000000  \n",
       "choice      0.000000   1.000000   1.000000   0.000000  \n",
       "cost        2.237128   1.855451   5.794677   2.137454  \n",
       "time       67.181889  60.293126  22.547429  23.255171  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the resulting long format dataframe\n",
    "long_mode_choice.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Divide the final dataset into a train and test subsets\n",
    "\n",
    "In machine learning it is a typical practise to divide the data into two sets: the training set and the test set. As these names imply, the model is trained using the training set, and then it is tested using the test set. This is commonly used to obtain the accuracy of the classifier on the new cases (the observations in the test set), which is called generalization error. This value indicates how well the model performs on instances it has never seen before. PyKernelLogit allows to divide a long format dataframe into a train and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentaje of total observations which are used for training. \n",
    "train_size = 0.7\n",
    "\n",
    "(train_set, test_set) = pkl.divide_long_train_test(long_mode_choice, train_size, obs_id_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimating a linear Random Utility model using a conditional logit model\n",
    "\n",
    "Firstly, a linear Random Utility model using a conditional logit model will be estimated using the Mode dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Definition of a conditional logit model\n",
    "\n",
    "Before estimating a model, it is necessary to pre-compute all of the variables that are going to be used in the model. This is different from the functionalities of other package such as mlogit or statsmodels that use formula strings to create new variables. This is also somewhat different from Python Biogeme where new variables can be defined in the script but not actually created by the user before model estimation. PyKernelLogit (as it contains the same internal structure as PyLogit) does not perform variable creation. It only estimates models using variables that already exist.\n",
    "\n",
    "For the Mode dataset no variable preprocessing was necessary. Moreover, it was decided not to scale any of the variables. No new variables have been defined either.\n",
    "\n",
    "The first step is to create the model specification being used in this example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_{\\textrm{Car}, n} &= \\beta_{\\textrm{travel cost}} \\cdot \\textrm{Cost}_{\\textrm{Car}, n} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel time}} \\cdot \\textrm{Time}_{\\textrm{Car}, n} \\\\\n",
    "        V_{\\textrm{Carpool}, n} &= \\textrm{ASC Carpool} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel cost}} \\cdot \\textrm{Cost}_{\\textrm{Carpool}, n} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel time}} \\cdot \\textrm{Time}_{\\textrm{Carpool}, n} \\\\\n",
    "        V_{\\textrm{Bus}, n} &= \\textrm{ASC Bus} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel cost}} \\cdot \\textrm{Cost}_{\\textrm{Bus}, n} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel time}} \\cdot \\textrm{Time}_{\\textrm{Bus}, n} \\\\\n",
    "        V_{\\textrm{Rail}, n} &= \\textrm{ASC Rail} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel cost}} \\cdot \\textrm{Cost}_{\\textrm{Rail}, n} + \\\\ \n",
    "        & \\quad \\beta_{\\textrm{travel time}} \\cdot \\textrm{Time}_{\\textrm{Rail}, n}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that packages such as mlogit and statsmodels do not, by default, handle coefficients that vary over some alternatives but not all. PyKernelLogit does have this capability.\n",
    "\n",
    "In order to specify a linear model in PyKernelLogit it is necessary to create two ordered dictionaries:\n",
    "\n",
    "* Specification ordered dictionary. It contains the desired PyKernelLogit model specification. The dictionary keys should be variables within the long format dataframe (i.e. the column names from the dataframe), whereas, their associate value should be a list of integers, where the integers are the alternative ID's of the alternative whose utility specification the explanatory variable is entering. In other words, for each integer $i$ in the list, a $\\beta_{ik}$ parameter is created, which multiplies the attribute $x_{ink}$ selected on the ordered dictionary key. It is also possible to use the same $\\beta_{k}$ parameter for more than one alternative. In this case, a nested list should be added containing the alternative ID's of the alternatives that will share the common parameter.\n",
    "    \n",
    "* Variable names ordered dictionary. This order dictionary should contain the same keys that the Specification ordered dictionary. The value should be a list with the names for the $\\beta_{k}$ parameters that will be estimated for each of the elements within the outer-most list on the Specification ordered dictionary.\n",
    "\n",
    "\n",
    "The keys of both ordered dictionaries should be variables from the long format dataframe with the sole exception of the intercept key. The value associated with this key is a list with the alternative ID's for which an intercept will be added in the model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty basic_specification and basic_names ordered dictionary\n",
    "basic_specification = OrderedDict()\n",
    "basic_names = OrderedDict()\n",
    "\n",
    "# Model specification\n",
    "basic_specification[\"intercept\"] = [2, 3, 4]\n",
    "basic_names[\"intercept\"] = ['ASC CarPool',\n",
    "                            'ASC Bus',\n",
    "                            'ASC Rail']\n",
    "\n",
    "basic_specification[\"cost\"] = [[1, 2, 3, 4]]\n",
    "basic_names[\"cost\"] = ['Travel cost (dollars)']\n",
    "\n",
    "basic_specification[\"time\"] = [[1, 2, 3, 4]]\n",
    "basic_names[\"time\"] = ['Travel time (minutes)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Estimation of the previous model\n",
    "\n",
    "Once the model has been specified, it is necessary to estimate its parameters. PyKernelLogit allows to estimate the previous model using Maximum Likelihood Estimation (MLE) or Penalized Maximum Likelihood Estimation (PMLE). In this example, as the model is not complex, MLE is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the multinomial logit model (MNL)\n",
    "linear_model = pkl.create_choice_model(data=train_set,\n",
    "                                           alt_id_col=custom_alt_id,\n",
    "                                           obs_id_col=obs_id_column,\n",
    "                                           choice_col=choice_column,\n",
    "                                           specification=basic_specification,\n",
    "                                           model_type=\"MNL\",\n",
    "                                           names=basic_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous model can be estimated using the fit_mle() method. This method is designed to estimate Multinomial Logit Models and receives several parameters: \n",
    "* init_vals. The initial values of $\\boldsymbol{\\beta}$ (RUM) or $\\boldsymbol{\\alpha}$ (KLR) to start the optimization process with. It should be a vector with one value for each utility coefficient being estimated.\n",
    "\n",
    "* method. It should be a valid string that can be passed to scipy.optimize.minimize. Determines the optimization algorithm that is used for this problem. The default newton-cg which uses the Newton-CG algorithm.\n",
    "\n",
    "* PMLE. It determines if a Penalized Maximum Likelihood Estimation should be executed. The value of the parameter determines the type of PMLE and the possible values are LASSO and RIDGE.\n",
    "\n",
    "* PMLE_lambda. Lambda parameter for LASSO or ridge regression. It should be an int, float or long and determines the penalty parameter for the PMLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -439.4553\n",
      "Initial Log-likelihood: -439.4553\n",
      "Estimation Time for Point Estimation: 0.01 seconds.\n",
      "Final log-likelihood: -246.0394\n"
     ]
    }
   ],
   "source": [
    "# Fit the linear model using MLE\n",
    "linear_model.fit_mle(np.zeros(5), method='BFGS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluation of the estimated model\n",
    "\n",
    "Once the model has been estimated, the next step is to asses the performance of the model.\n",
    "\n",
    "In machine learning, there are several alternatives for assessing whether a model is good enough for the specific application for which the model is going to be used. The goodness of fit of a model is a statistic which descries how well a model, with its estimated parameters, performs compared with a model in which all its parameters are zero (which is usually equivalent to having no model at all, i.e. in the discrete choice context is equivalent to a model for which the probability of choosing an alternative is the same for all the alternatives). This comparison is made using the log-likelihood function, which is evaluated at both models, one with estimated parameters and other with all its parameters equal zero. The likelihood ratio index (also called McFadden R square) is defined as:\n",
    "\\begin{equation}\n",
    "    \\label{eq:ro-square}\n",
    "    \\rho^2 = 1-\\frac{LL(\\hat{\\boldsymbol{\\beta}})}{LL(\\boldsymbol{0})}.\n",
    "\\end{equation}\n",
    "\n",
    "This measure has been derived to mimic the $R^2$ used in linear regression, but in this case its interpretation is not similar at all. If the estimated model has the same log-likelihood as an equal probability model, then $\\rho^2 = 0$. On the contrary, if the estimated model perfectly fits the data ($LL(\\hat{\\boldsymbol{\\beta}}) = 0$), then $\\rho^2 = 1$. This value, unlike $R^2$, cannot be interpreted, and it must only be used to compare between two models estimated using the same sample. For this reason, two models estimated on different samples cannot be compared via their likelihood ratio index values.\n",
    "\n",
    "It presents another disadvantage because this measure is monotonic in the number of parameters of the model, which means that this index increase each time an additional variable is added to the model, even if it does not explain anything. There is a variant of the $\\rho^2$ index which offer a corrected measure:\n",
    "\\begin{equation}\n",
    "    \\label{eq:pseudo-ro-bar-square}\n",
    "    \\overline{\\rho}^{2} = 1-\\frac{LL(\\hat{\\boldsymbol{\\beta}})-\\nu}{LL(0)},\n",
    "\\end{equation}\n",
    "where $\\nu$ is the number of parameters in the model.\n",
    "\n",
    "When a model is estimated, PyKernelLogit allows to obtain a summary of the results by using the function get_statsmodels_summary. This function integrates both, $\\rho^2$ and $\\overline{\\rho}^{2}$ goodness of fit measures, which are called here as Pseudo R-squ. and Pseudo R-bar-squ., respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multinomial Logit Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>choice</td>          <th>  No. Observations:  </th>    <td>317</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>         <td>Multinomial Logit Model</td> <th>  Df Residuals:      </th>    <td>312</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>           <th>  Df Model:          </th>     <td>5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Oct 2019</td>     <th>  Pseudo R-squ.:     </th>   <td>0.440</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:01:35</td>         <th>  Pseudo R-bar-squ.: </th>   <td>0.429</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AIC:</th>                   <td>502.079</td>         <th>  Log-Likelihood:    </th> <td>-246.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BIC:</th>                   <td>520.873</td>         <th>  LL-Null:           </th> <td>-439.455</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC CarPool</th>           <td>   -4.0019</td> <td>    0.465</td> <td>   -8.614</td> <td> 0.000</td> <td>   -4.912</td> <td>   -3.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC Bus</th>               <td>   -2.9700</td> <td>    0.362</td> <td>   -8.201</td> <td> 0.000</td> <td>   -3.680</td> <td>   -2.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC Rail</th>              <td>   -2.4504</td> <td>    0.332</td> <td>   -7.387</td> <td> 0.000</td> <td>   -3.101</td> <td>   -1.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Travel cost (dollars)</th> <td>   -0.7246</td> <td>    0.107</td> <td>   -6.796</td> <td> 0.000</td> <td>   -0.934</td> <td>   -0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Travel time (minutes)</th> <td>   -0.0918</td> <td>    0.010</td> <td>   -9.551</td> <td> 0.000</td> <td>   -0.111</td> <td>   -0.073</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Multinomial Logit Model Regression Results                    \n",
       "===================================================================================\n",
       "Dep. Variable:                      choice   No. Observations:                  317\n",
       "Model:             Multinomial Logit Model   Df Residuals:                      312\n",
       "Method:                                MLE   Df Model:                            5\n",
       "Date:                     Wed, 02 Oct 2019   Pseudo R-squ.:                   0.440\n",
       "Time:                             20:01:35   Pseudo R-bar-squ.:               0.429\n",
       "AIC:                               502.079   Log-Likelihood:               -246.039\n",
       "BIC:                               520.873   LL-Null:                      -439.455\n",
       "=========================================================================================\n",
       "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "ASC CarPool              -4.0019      0.465     -8.614      0.000      -4.912      -3.091\n",
       "ASC Bus                  -2.9700      0.362     -8.201      0.000      -3.680      -2.260\n",
       "ASC Rail                 -2.4504      0.332     -7.387      0.000      -3.101      -1.800\n",
       "Travel cost (dollars)    -0.7246      0.107     -6.796      0.000      -0.934      -0.516\n",
       "Travel time (minutes)    -0.0918      0.010     -9.551      0.000      -0.111      -0.073\n",
       "=========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain a summary of the results for the linear model\n",
    "linear_model.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These goodness of fit measures cannot measure how the model predict on new data points, for which it has not yet seen the true value of the dependent variable (the alternative chosen). Therefore, some other model validation techniques are available on the PyKernelLogit package.\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the so-called confusion matrix. The idea is to count the number of instances of class A which are classified as class B. The confusion matrix can be computed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  2   3   4\n",
       "1  58  0   6   8\n",
       "2   6  0   1   3\n",
       "3   3  0  12   5\n",
       "4   8  0   8  18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the confunsion matrix using the test set\n",
    "confusion_matrix = linear_model.confusion_matrix(test_set)\n",
    "\n",
    "# The generated confusion matrix can be shown directly as a pandas dataframe\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Confusion Matrix')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFNCAYAAAAq3JTxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xl8XHW9//HXO0nbtNCFpUmBVopQQXa5LBXZCgoIKEUKyKKA1SiLVwURKjsKP7gogle5WvadssNlx0ova2WnLbIjS21p2bvQtE3y+f0xpzXUZjKZZuacybyfPM4jc86cOec9CdN88v1+z/coIjAzMzPLkpq0A5iZmZktywWKmZmZZY4LFDMzM8scFyhmZmaWOS5QzMzMLHNcoJiZmVnmuEAxqxCS+kr6X0mfSLpxBY5zsKT7uzNbGiTdI+nQtHOYWWm4QDHrZpIOkvSUpHmSZia/SLfrhkOPARqB1SJiv2IPEhHXRMSu3ZDnMyTtJCkk3bLM9s2S7ZMKPM5pkq7ubL+I+HpEXFFkXDPLOBcoZt1I0jHA+cBZ5IqJzwEXAnt3w+HXBl6JiJZuOFapvAdsK2m1dtsOBV7prhMox/92mfVw/pCbdRNJA4EzgKMi4paImB8RiyPifyPiuGSfPpLOlzQjWc6X1Cd5bidJ0yUdK2l20vpyePLc6cApwAFJy8zYZVsaJA1PWirqkvXDJL0haa6kf0g6uN32R9q9bltJTyZdR09K2rbdc5Mk/UrSo8lx7pe0ep5vwyLgNuDbyetrgf2Ba5b5Xl0g6R1JcyQ9LWn7ZPvuwC/bvc/n2+U4U9KjwKfA55Nt30+e/x9JN7U7/jmSJkpSwT9AM8sUFyhm3efLQD1wa559TgRGApsDmwFbAye1e34IMBBYCxgL/FHSKhFxKrlWmQkRsXJEXJIviKSVgN8DX4+I/sC2wHPL2W9V4K5k39WA84C7lmkBOQg4HGgAegM/z3du4Ergu8nj3YAXgBnL7PMkue/BqsC1wI2S6iPi3mXe52btXvMdoAnoD7y1zPGOBTZNiq/tyX3vDg3fy8OsYrlAMes+qwHvd9IFczBwRkTMjoj3gNPJ/eJdYnHy/OKIuBuYB6xfZJ42YGNJfSNiZkS8sJx99gRejYirIqIlIq4DXgK+0W6fyyLilYhYANxArrDoUEQ8BqwqaX1yhcqVy9nn6oj4IDnnb4E+dP4+L4+IF5LXLF7meJ8Ch5ArsK4GfhwR0zs5npllmAsUs+7zAbD6ki6WDqzJZ//6fyvZtvQYyxQ4nwIrdzVIRMwHDgB+BMyUdJekDQrIsyTTWu3W3y0iz1XA0cAoltOilHRjvZh0K31MrtUoX9cRwDv5noyIJ4A3AJErpMysgrlAMes+jwPNwOg8+8wgN9h1ic/x790fhZoP9Gu3PqT9kxFxX0R8DViDXKvIRQXkWZLpn0VmWuIq4Ejg7qR1Y6mkC+Z4cmNTVomIQcAn5AoLgI66ZfJ210g6ilxLzAzgF8VHN7MscIFi1k0i4hNyA1n/KGm0pH6Sekn6uqT/Sna7DjhJ0uBksOkp5LokivEcsIOkzyUDdMcteUJSo6RvJmNRFpLrKmpdzjHuBr6QXBpdJ+kAYEPgziIzARAR/wB2JDfmZln9gRZyV/zUSToFGNDu+VnA8K5cqSPpC8CvyXXzfAf4haS8XVFmlm0uUMy6UUScBxxDbuDre+S6JY4md2UL5H6JPgVMAaYCzyTbijnXA8CE5FhP89mioobcwNEZwIfkioUjl3OMD4C9kn0/INfysFdEvF9MpmWO/UhELK916D7gHnKXHr9FrtWpfffNkknoPpD0TGfnSbrUrgbOiYjnI+JVclcCXbXkCikzqzzyIHczMzPLGregmJmZWea4QDEzM7PMcYFiZmZmmeMCxczMzDLHBYqZmZllTr4ZL9Pmy4vMzKzalPUGl7VfPrDLv2tbH7+uLBmzXKBQ++UD045gRWh9/Lrcg+Z8t6SxzKqv45P35qedwoowcPBKAMyeMSflJFaMhjUHdL5TFcl0gWJmZmYlVPiEzWXnAsXMzKxaqaw9Sl3iAsXMzKxauQXFzMzMMscFipmZmWWOu3jMzMwsc9yCYmZmZpnjAsXMzMyyRjXu4jEzM7OscQuKmZmZZY4LFDMzM8scX8VjZmZmmeMWFDMzM8scFyhmZmaWOe7iMTMzs8xxC4qZmZllToYLlOwmMzMzs6rlFhQzM7Nq5TEoZmZmljkZ7uJxgWJmZlatXKCYmZlZ5riLx8zMzDLHLShmZmaWOS5QzMzMLHNq3MVjZmZmGSO3oJiZmVnmuEAxMzOzzPFVPNXt9Vt+z9xPF9Da2kZLaxvbfO9ENhuxNhf+Yiz1vXvR0trG0b+5lCf//nraUa0TDz36MGeeczZtba3st8++NI39QdqRrAvmzp3LmeecwetvvI4EJ407lU033iztWFaACTdey5133YYkPv/59Rh3/Cn06d0n7ViVzy0otstRv+aDT+YuXT/nqIP41SU3c+/k5/n6lzfn7KMOYpejfpViQutMa2srZ5x1Jpf9+SIaGxsZc9AB7LzTKNZbd720o1mBfnvBuYzcZlvO/vW5LF68mObm5rQjWQHee282N98ygasun0CfPvWccto4Jv71fvbY/RtpR6t8GS5Qspush4sIBqzUF4CBK/dj5vsfpZzIOjNl2lTWHjaMYUOH0btXb/bcfQ8mTnow7VhWoHnz5/Hs88+w916jAejVqxf9+/dPOZUVqrW1hYULF9LS2kLzwmZWX21w2pF6BqnrS5m4BaUMIoJ7LxhHRHDRbRO56Pa/8rPzr+Se88fxXz8+hJoasV3TqWnHtE7Mmj2LIUPWWLre2NDIlKlTUkxkXTFjxj9ZZdAqnHHWabz62itssP4XOfYnx9G3b9+0o1knBg9u4Nv7H8KYA75B7z592HrLbdh6q5Fpx+oZ3ILyL5IOL/c507b9D09jq8N+yZ7HnMMR++7K9ptvwI++9TWOveAqho8+mmMvuIqLftmUdkzrRMS/b1OGB5jZZ7W0tvLyKy+x7+gxXH3ZdfSt78sVV1+WdiwrwNy5c3jksYeYcN3t3HbTPSxobua+B+5OO1bPoJquL2WSRul0ekdPSGqS9JSkp8aPH1/OTCW1pPvmvY/mcNv/PclWG67Ld/fYgVsmPQHAjRMns/WG66YZ0QowpLGRd9+duXR91uxZNDQ0pJjIuqJhcAMNgxvYeKNNANh51C68/MpLKaeyQjz19BOsMWRNVhm0CnV1dey4/SimTXPrZbcoURePpDclTZX0nKSnkm2rSnpA0qvJ11XyHaMkBYqkKR0sU4HGjl4XEeMjYsuI2LKpqWe0KPSr78PK/eqXPv7aNpvywhvTmfH+R+z4pS8CsPOWG/HqO++mGdMKsMlGG/Pm22/zzvTpLFq8iLvuvZuddxyVdiwr0OqrrU5DQyNvvf0mAE8+9QTrDF8n3VBWkIaGIbzw96k0NzcTETz9zJOsvbZ/dhVgVERsHhFbJusnABMjYgQwMVnvUKnGoDQCuwHLjvwU8FiJzplJjasO5OazjwGgrraW6+5/lPsmP8+8T5v53c++S11tLc2LFvOjsy9OOal1pq6ujlPGncj3j2iita2NfUfvw4j1fAVPJTnuZ8dz8ukn0tKymDXXHMop405LO5IVYKMNN2anHXdhbNMh1NbWMmLE+nxzr33SjtUzlLebem9gp+TxFcAk4PiOdlYsr2N9BUm6BLgsIh5ZznPXRsRBBRwmar98YLdns9Jrffy63IPmlnSDWHHq6/jkvflpp7AiDBy8EgCzZ8xJOYkVo2HNAZD7Q75seh1yfpeLgJZrfvZDoH03x/iI+My4DEn/INdIEcCfI2K8pI8jYlC7fT6KiA67eUrSghIRY/M8V0hxYmZmZqVWRAtKUox0NlD0KxExQ1ID8ICkLg/48mXGZmZm1apEV+VExIzk62xJtwJbA7MkrRERMyWtAczOd4zsXgBtZmZmpVWCq3gkrSSp/5LHwK7ANOAO4NBkt0OB2/Mdxy0oZmZmVaskQ14agVuTeaLqgGsj4l5JTwI3SBoLvA3sl+8gLlDMzMyqVQmu4omIN4B/uwtnRHwA7FLocVygmJmZVasMz4btAsXMzKxquUAxMzOzrHELipmZmWWOCxQzMzPLHhcoZmZmljVuQTEzM7PMKdFMst3BBYqZmVnVcguKmZmZZYwy3MWT3bYdMzMzq1puQTEzM6tWGW5BcYFiZmZWrVygmJmZWfa4QDEzM7OscQuKmZmZZY4LFDMzM8seFyhmZmaWNW5BMTMzs8xxgWJmZmbZk935Wl2gmJmZVSu3oJiZmVnmuEAxMzOz7HGBYmZmZlnjFhQzMzPLHBcoxWl9/Lq0I9iKqM/0/16Wx8DBK6UdwVZAw5oD0o5gFcMFipmZmWWNW1CK88l789OOYEVY+td3c0u6Qaw49XW0fbo47RRWhJp+vQBo/qQ55SRWjPqB9WlHyJRMFyhmZmZWOjVuQTEzM7OsyXB94gLFzMysWrkFxczMzDKnJrv1iQsUMzOzaiW3oJiZmVnWuAXFzMzMMsctKGZmZpY5bkExMzOzzHELipmZmWWOW1DMzMwsc7LcglKTdgAzMzNLR426vhRKUq2kZyXdmayvI+lvkl6VNEFS77zZVuytmZmZWaWS1OWlC34CvNhu/RzgdxExAvgIGJvvxS5QzMzMqlSpWlAkDQX2BC5O1gXsDNyU7HIFMDpvtmLflJmZmVW2YlpQJDVJeqrd0rScQ58P/AJoS9ZXAz6OiJZkfTqwVr5sHiRrZmZWpYq5iicixgPjO3pe0l7A7Ih4WtJOSzYv71D5zuMCxczMrEqV6G7GXwG+KWkPoB4YQK5FZZCkuqQVZSgwI2+2UiQzMzOz6hQR4yJiaEQMB74N/DUiDgYeBMYkux0K3J7vOC5QzMzMqpTU9WUFHA8cI+k1cmNSLsm3s7t4zMzMqlSJuniWiohJwKTk8RvA1oW+1gWKmZlZlcrwRLIuUMzMzKpVqVtQVoQLFDMzsyqV4frEBYqZmVm1cguKmZmZZU6G6xMXKGZmZtXKLShmZmaWORmuTworUCR9G1g3Is6UNAxoiIinSxut55o7dy5nnnMGr7/xOhKcNO5UNt14s7RjWQEeevRhzjznbNraWtlvn31pGvuDtCNZgU487SQmPfQQq666Kv97021px7EuWLhwIYf/8HAWL1pMS2sLX9vlaxzZdGTasXqELLegdDqTrKQ/AKOAQ5JN84E/lTJUT/fbC85l5DbbcuO1t3DN5RNYZ+3Ppx3JCtDa2soZZ53JxRf+ibtuvYM7772b115/Le1YVqDR3xjN+D/6n65K1Lt3by6+8GJuvPZGbrjmBh59/FGmTJ2SdqweocwzyXZJIVPdbxsRPwSaASLiQ6B3SVP1YPPmz+PZ559h771GA9CrVy/69++fciorxJRpU1l72DCGDR1G71692XP3PZg46cG0Y1mBtvqPLRk0cGDaMawIkujXrx8ALS0ttLS0LP/euNZlNVKXl7JlK2CfxZJqSG6LLGk1oK2zF0naQNIuklZeZvvuRSXtIWbM+CerDFqFM846jUMOP5Bfn30GCxYsSDuWFWDW7FkMGbLG0vXGhkZmzZqVYiKz6tHa2sr+B+/PqN1GMXLrkWy68aZpR+oRatT1pWzZCtjnj8DNwGBJpwOPAOfke4Gk/yR3l8IfA9Mk7d3u6bOKzNojtLS28vIrL7Hv6DFcfdl19K3vyxVXX5Z2LCtAxL9vU4b7b816ktraWm645gbuv/N+pv19Gq++/mrakXoESV1eyqXTAiUirgROAn4DfATsFxHXd/KyHwD/ERGjgZ2AkyX9JHmuw3cnqUnSU5KeGj9+fCH5K07D4AYaBjew8UabALDzqF14+ZWXUk5lhRjS2Mi7785cuj5r9iwaGhpSTGRWfQb0H8BWW2zFY48/lnaUHqGiW1Ak/RZYKSIuiIjzI2JaAcetjYh5ABHxJrki5euSziNPgRIR4yNiy4jYsqmpqaA3UGlWX211GhoaeevtNwF48qknWGf4OumGsoJsstHGvPn227wzfTqLFi/irnvvZucdR6Udy6zH+/CjD5kzdw4Azc3NTH5iMsPXHp5uqB4iy2NQCrnM+O/AryUNJ9fVMyEinuvkNe9K2nzJfhExT9JewKXAJiuQt0c47mfHc/LpJ9LSspg11xzKKeNOSzuSFaCuro5Txp3I949oorWtjX1H78OI9dZLO5YV6NgTjuOJp5/k448/ZqfdduHoHx3JmH32TTuWFeD999/npNNPoq2tjba2Nnb96q7suP2OaceyElMsr2N9eTtKg4ExwAHAkIjYIM++Q4GWiHh3Oc99JSIeLeCU8cl78wvKZtkycPBKuQfNLekGseLU19H26eK0U1gRavr1AqD5k+aUk1gx6gfWQ5mvT9ruvx8qrAho55Ef71CWjF2ZSXYYMBxYC8g7+UNETM/zXCHFiZmZmZVYlidq67RAkXQmuZaTd4AJwDbJXChmZmZWwQq5lDcthbSgzAR2iAhP+GBmZtaDZHmqhA4LFEkjIuJV4CGgUVJj++cjwvMMm5mZVbByXjbcVflaUE4AxpKbqG1ZAexQkkRmZmZWFhVZoETE2OThzhHxmSH9knqVNJWZmZmVXJYHyRYyPuZvBW4zMzOzClJTxFIu+cagNABrAH0lbcK/rs0eAPQrQzYzMzMroSy3oOQbg7In8D1gKHBhu+1zgZNLGcrMzMxKr1LHoFwGXCZp/4i4oYyZzMzMrAwqtQUFgIi4QdJuwEZAfbvtZ5UymJmZmZVWRU/UJulCYBC5y4ovA/YFJpc4l5mZmZVYlltQCimetouIg4APIuJkYBty41LMzMysgtWo60u5FDLV/YLka7OkIcAH5G4aaGZmZhUsyy0ohRQo90gaBPwGeA5oBa4saSozMzMruYq8imeJiDgteXijpDuBvr6bsZmZmZVSIS0oSNqaXLdOXbJORFxbwlxmZmZWYjVktwmlkKt4Lgc25F/dO5C7WaALFDMzswpW0V08wEhgw4hoK3UYMzMzK59KHyT7ArA6MLvEWczMzKyMKr0FZSDwoqTJwMIlGyPiWyVLZWZmZiVX6S0o/6/kKczMzKzsSlGeSKoHHgL6kKszboqIUyWtA1wPrAo8A3wnIhZ1dJy8BYqkWuAXEbFbtyU3MzOzTChRF89CYOeImCepF/CIpHuAY4DfRcT1kv4EjAX+p8Ns+c4QEa3AIkkDujG4mZmZZUCN1OWlM5EzL1ntlSwB7AzclGy/Ahid7ziFdPHMA56XdD8wv12AYwp4rZmZmWVUqQbJJj0wTwPrAX8EXgc+joiWZJfpwFr5jlFIgfKXZDEzM7MepJiJ2iQ1AU3tNo2PiPHt90l6YDZPbpVzK/DF5Rwq8p2nkKnuL5HUG/hcRLzWaXIzMzOrCMW0oCTFyPhOd8zt+7GkSeTmVBskqS5pRRkKzMibrbODS9oTmAo8kKxvLunWQoKZmZlZdpViDIqkwUnLCZL6Al8FXgQeBMYkux0K3J7vOIV08ZwBbJMcmIh4TtJ6BbzOzMzMMqxEY1DWAK5IxqHUADdExJ2S/g5cL+nXwLPAJfkOUkiBsjhpomm/LW+/UXcZOHilcpzGSqW+oHtRWgbV9OuVdgRbAfUD69OOYBWiFBO1RcQU4EvL2f4GsHWhxynkN8iLkvYHapJJVn4CTC70BGZmZpZNnY7zSFEhBcrRwClAG3ALcB8wrpShlmj+pLkcp7FutvSvt+aW/DtaNtXX8eor76Wdwoow4guDAZj7wacpJ7Fi9F+tX9nPqQqf6n63iDgeOH7JBknfIlesmJmZmXW7Qlp3TlrOthO7O4iZmZmVV426vpRLhy0oknYDdgfWknReu6cGkOvuMTMzswqW3Q6e/F08s4EXgObk6xJzgRNKGcrMzMxKrxRX8XSXDguUiHgWeFbSVRGxsIyZzMzMrAwq8ioeSc+SzHeyvFG+EbFF6WKZmZlZqWW4ASVvF8+YPM+ZmZlZhavULp7XyxnEzMzMyiu75UlhNwvcStJkSZ9Iapa0UNKccoQzMzOz0inFzQK7SyETtV0IHAJcT24O/cOAYSXMZGZmZmWQ4R6eggbw1kTEy0BdRCyOiIvI3TrZzMzMKlhNEUu5FNKCMl9Sb+B5SWcBM4GVSxvLzMzMSi3Lg2QLKYYOS/Y7GmgFRuArfMzMzCqeiljKpdMWlIh4I3nYDJxc2jhmZmZWLuW8t05XFdLFY2ZmZj3Q8iZizQoXKGZmZlWqIqe6X5akPr4nj5mZWc+R5RaUQiZq21rSVODVZH0zSf9d8mRmZmZWtQpp3fk9sBfwAUBEPA+MKmUoMzMzK70adX0pl0K6eGoi4q1lmoFaS5THzMzMyqTSx6C8I2lrICTVAj8GXiltLDMzMyu1LI9BKaRAOYJcN8/ngFnAX5JtZmZmVsEqugUlImYD3y5DFjMzMyujDDegdF6gSLoIiGW3R0RTSRKZmZlZWWT5XjyFdPH8pd3jemAf4J3SxDEzM7NyyW55UlgXz4T265KuAh4oWSIzMzMri0pvQVnWOsDa3R3EzMzMyquibxYo6SP+NQalBvgQOKGUoXqyhQsXcvgPD2fxosW0tLbwtV2+xpFNR6Ydywr00KMPc+Y5Z9PW1sp+++xL09gfpB3J8jj/grN48snHGDhwFS7841UAXHrpH3niiUep69WLIUPW5Kc/+SUrr9w/5aTWmW98aw/69VuJ2toaamtruerSa9OO1CNkuD7JX6Aod4H0ZsA/k01tEfFvA2atcL179+biCy+mX79+LG5ZzGE/OIztvrwdm26yadrRrBOtra2ccdaZXPbni2hsbGTMQQew806jWG/d9dKOZh346i57sNee+3Le7369dNvmm2/FoYf+kNraOi67/EJuvOkqDj/MfyRUgj//YTyDBq2SdoweJctdPHkvgU6KkVsjojVZCi5Oknv4bJU83lDSMZL2WMG8FU8S/fr1A6ClpYWWlpZsl7C21JRpU1l72DCGDR1G71692XP3PZg46cG0Y1keG2+8Of37D/jMti222Jra2tzfZuuvvxHvv/9eGtHMMkHq+lIuhYxBeULSFhHxTKEHlXQq8HWgTtIDwDbAJOAESV+KiDOLSttDtLa2cuB3D+Tt6W9zwJgD2HRjt55UglmzZzFkyBpL1xsbGpkydUqKiWxFPfDAXeyw/S5px7ACSOKonx6JJL619758a/S+aUfqESpyojZJdRHRAmwH/EDS68B8cn/vR0Rskee4Y4DNgT7Au8DQiJgj6Vzgb0BVFyi1tbXccM0NzJk7h5/94me8+vqrjFh3RNqxrBPLaz/M8jTRlt+ECVdQW1vLTjvtmnYUK8Alf7qMwYMb+PDDDznqpz9i+NrD2eJL/5F2rIqX5X/D8hVPTyRfRwPrA3sA+5ErPvbr5LgtSZfQp8DrETEHICIWAG0dvUhSk6SnJD01fvz4Qt9DxRrQfwBbbbEVjz3+WNpRrABDGht5992ZS9dnzZ5FQ0NDiomsWBMn3sMTTz7Gz489NdP/QNu/DB6c+6ytuuqq7LTDzrzw4gspJ+oZaopYypmtIwKIiNeXt3Ry3EWS+iWPl5a4kgaSp0CJiPERsWVEbNnU1DMnqv3wow+ZM3cOAM3NzUx+YjLD1x6ebigryCYbbcybb7/NO9Ons2jxIu6692523nFU2rGsi55+ejI33XwNp5x8NvX19WnHsQIsWLCA+fPnL338tyceZ93Pr5tyqp5BUpeXcsk3BmWwpGM6ejIizsvz2h0iYmGyX/uCpBdwaNci9izvv/8+J51+Em1tbbS1tbHrV3dlx+13TDuWFaCuro5Txp3I949oorWtjX1H78OI9XwFT5b917mnMnXqc8yZ8zGHHrYPBx80lhtvuorFixdz0sk/A3IDZY8+6riUk1o+H3z4AceNy/06am1tZbevfZ1tR34l5VQ9Q5YbENXRhTmSZgL/QwfXmETE6SXMBRDNnzSX+BRWCvUDk79Km1vSDWLFqa/j1Vd8ZUslGvGFwQDM/eDTlJNYMfqv1g/KfF3nDZNe7/LUIfvvtG5ZMuZrQZkZEWeUI4SZmZmVXynGlEgaBlwJDCE3rGN8RFwgaVVgAjAceBPYPyI+KiZbhht+zMzMbEWVaAxKC3BsRHwRGAkcJWlDcrPQT4yIEcBEOpmVPl+B4skBzMzMerBSTNQWETOXzJ0WEXOBF4G1gL2BK5LdriB3lXCHOuziiYgPC3t7ZmZmVolKfdmwpOHAl8jNgdYYETMhV8RIyjtPQ5YnkTMzM7MSKqaLp/2cZcmy3HlBJK0M3Az8dMl8aF1RyFT3ZmZm1gMVM9g0IsYDeWdTldSLXHFyTUTckmyeJWmNpPVkDWB2vmO4BcXMzKxK1ajrS2eUG0l7CfDiMnOm3cG/5kI7FLg933HcgmJmZlalSjQz7FeA7wBTJT2XbPslcDZwg6SxwNt0ctscFyhmZmZVqhTlSUQ8kufQBV8h7ALFzMysShXSZZMWFyhmZmZVKst383aBYmZmVqWyW564QDEzM6ta7uIxMzOzzFGG21BcoJiZmVWpDA9BcYFiZmZWrbLcxeOZZM3MzCxz3IJiZmZWpTwGxczMzDLHY1DMzMwsc1ygmJmZWebUuIvHzMzMssYtKGZmZpY5Ga5PXKCYmZlVq5oMN6G4QDEzM6tSGa5PXKCYmZlVK8+DYmZmZpmT5anuXaCYmZlVKXfxmJmZWea4i6dI9QPr045gK6I+0/97WR4jvjA47Qi2Avqv1i/tCFYh3MVjZmZmmeMuniLNnjEn7QhWhIY1B+QeNLekG8SKU1/nz16FWvLZmzj57ZSTWDF2Gfm5sp/TXTxmZmaWPdmtT6hJO4CZmZnZstyCYmZmVqU8BsXMzMwyx2NQzMzMLHPcgmJmZmaZowxXKC5QzMzMqlR2yxMXKGZmZlUrww0oLlDMzMyqVZa7eDwPipmZmWWOW1DMzMyqVIYbUFygmJmZVassd/G4QDEzM6tS2S1PXKCYmZlVLbegmJmZWeZkuD7xVTxmZmbVSur60vkxdamk2ZKmtdu2qqQHJL2afF2ls+O4QDEzM6tSKuK/AlwO7L5aKNV4AAAMiElEQVTMthOAiRExApiYrOflAsXMzKxaqYilExHxEPDhMpv3Bq5IHl8BjO7sOC5QzMzMrNQaI2ImQPK1obMXuEAxMzOrUsWMQZHUJOmpdktTKbL5Kh4zM7MqVeCYks+IiPHA+C6+bJakNSJipqQ1gNmdvcAtKGZmZlWqFFfxdOAO4NDk8aHA7Z29wC0oZmZmVaoUE7VJug7YCVhd0nTgVOBs4AZJY4G3gf06O44LFDMzsypVinnaIuLADp7apSvHcYFiZmZWpbI8k6wLFDMzsyqV5XvxeJCsmZmZZY5bUFIw4cZrufOu25DE5z+/HuOOP4U+vfukHcsK8NCjD3PmOWfT1tbKfvvsS9PYH6QdybrAn73KctXFv2Hqc3+j/4BBnHzWRQC889ZrXHfFBbQsXkRNTS3f/u5/MnzdDVJOWrky3IDiFpRye++92dx8ywQu/vOVXHnZBNpa25j41/vTjmUFaG1t5YyzzuTiC//EXbfewZ333s1rr7+WdiwrkD97lWfkdrty9M/P+sy2WydcxJ57f4df/urP7PWtQ7n1hotSStczSOryUi5lK1AkXVmuc2Vda2sLCxcupKW1heaFzay+2uC0I1kBpkybytrDhjFs6DB69+rNnrvvwcRJD6Ydy7rAn73KMmKDTVlppf6f2SaJBc2fArDg0/kMHLRaGtF6jDLOg9JlJenikXTHspuAUZIGAUTEN0tx3koweHAD397/EMYc8A169+nD1ltuw9ZbjUw7lhVg1uxZDBmyxtL1xoZGpkydkmIi6wp/9nqGMQcfwR/OHcct148n2tr4+ckXpB2pohUzk2y5lKoFZSgwBzgP+G2yzG33uGrNnTuHRx57iAnX3c5tN93DguZm7nvg7rRjWQEi/n1blkfA22f5s9czPPzXOxlz0BGc9btrGXPQEVx9SVX/SllhWW5BKVWBsiXwNHAi8ElETAIWRMT/RcT/dfSi9jcgGj++q9P8V4annn6CNYasySqDVqGuro4dtx/FtGn+K7wSDGls5N13Zy5dnzV7Fg0Nnd6Q0zLCn72eYfIj97P5ltsBsMXWO/DWGy+nnKiyqYilXEpSoEREW0T8DjgcOFHSHyigOykixkfElhGxZVNTSW6OmLqGhiG88PepNDc3ExE8/cyTrL32OmnHsgJsstHGvPn227wzfTqLFi/irnvvZucdR6Udywrkz17PMHDQarz6Uq6wfPnvzzK4ca2UE1W4DDehlPQy44iYDuwnaU9yXT5Vb6MNN2anHXdhbNMh1NbWMmLE+nxzr33SjmUFqKur45RxJ/L9I5pobWtj39H7MGK99dKOZQXyZ6/yXHrhmbzy0hTmzfuEX/70QPbc57sc/L1juPHqC2lra6VXr94cfPhP045Z0bLcSa1YXsd6NsTsGa5pKlHDmgNyD5pb0g1ixamvw5+9yrTkszdx8tspJ7Fi7DLyc1DmmuGjWfO6XASs0rhyWTJ6ojYzM7NqleEmFBcoZmZmVSrD9YkLFDMzs2qV5akSPNW9mZmZZY5bUMzMzKpUhhtQXKCYmZlVr+xWKC5QzMzMqlSWW1A8BsXMzMwyxy0oZmZmVSrLLSguUMzMzKqUPAbFzMzMMie79YkLFDMzs2qV4frEBYqZmVnVynCF4gLFzMysSnkMipmZmWVOlq/i8TwoZmZmljluQTEzM6tSvpuxmZmZWRe4QDEzM7PMcRePmZlZlcpwD48LFDMzs2qV4frEBYqZmVnVynATigsUMzOzKpXd8sQFipmZWfXKcIXiAsXMzKxKeap7MzMzy5wMD0HxPChmZmbWvSTtLullSa9JOqGYY7hAMTMzq1JS15fOj6la4I/A14ENgQMlbdjVbC5QzMzMqpaKWDq1NfBaRLwREYuA64G9u5os02NQGtYckHYEWxH1mf7fy/LwZ6+y7TLyc2lHsApRojEoawHvtFufDmzT1YNk+TdIhofurDhJTRExPu0cVhz//CqXf3aVzT+/blZf1+XftZKagKZ2m8Yv8zNZ3jGjq+dxF096mjrfxTLMP7/K5Z9dZfPPL2URMT4itmy3LFswTgeGtVsfCszo6nlcoJiZmVl3ehIYIWkdSb2BbwN3dPUgWe7iMTMzswoTES2SjgbuA2qBSyPiha4exwVKetyHWtn886tc/tlVNv/8KkBE3A3cvSLHUESXx62YmZmZlZTHoJiZmVnmuEApM0mXSpotaVraWaxrJA2T9KCkFyW9IOknaWeywkmql/SEpOeTn9/paWeyrpFUK+lZSXemncVKzwVK+V0O7J52CCtKC3BsRHwRGAkcVcz0zZaahcDOEbEZsDmwu6SRKWeyrvkJ8GLaIaw8XKCUWUQ8BHyYdg7ruoiYGRHPJI/nkvuHcq10U1mhImdestorWTwIr0JIGgrsCVycdhYrDxcoZkWQNBz4EvC3dJNYVyRdBM8Bs4EHIsI/v8pxPvALoC3tIFYeLlDMukjSysDNwE8jYk7aeaxwEdEaEZuTm9lya0kbp53JOidpL2B2RDyddhYrHxcoZl0gqRe54uSaiLgl7TxWnIj4GJiEx4NViq8A35T0Jrk74+4s6ep0I1mpuUAxK5AkAZcAL0bEeWnnsa6RNFjSoORxX+CrwEvpprJCRMS4iBgaEcPJTZv+14g4JOVYVmIuUMpM0nXA48D6kqZLGpt2JivYV4DvkPvr7blk2SPtUFawNYAHJU0hd6+QByLCl6uaZZRnkjUzM7PMcQuKmZmZZY4LFDMzM8scFyhmZmaWOS5QzMzMLHNcoJiZmVnmuEAx6waSWpPLjqdJulFSvxU41k5L7tYq6ZuSTsiz7yBJRxZxjtMk/bwL+x8m6Q/J49HlukmipF8us/5YOc5rZulzgWLWPRZExOYRsTGwCPhR+yeV0+XPW0TcERFn59llENDlAmUFjQa6VKBIqivyXJ8pUCJi2yKPY2YVxgWKWfd7GFhP0nBJL0q6EHgGGCZpV0mPS3omaWlZGUDS7pJekvQI8K0lB1qm5aJR0q2Snk+WbYGzgXWT1ptzk/2Ok/SkpCmSTm93rBMlvSzpL8D6ywsu6RuS/ibpWUl/kdS4zPPbAt8Ezk3OuW6y3CvpaUkPS9og2fdySedJehA4J2m1uVTSJElvSPrPdse9LXn9C5Kakm1nA32T81yTbJuXfJ3QfpK85Fz7JjcDPLfd+/9hUT9BM0tfRHjx4mUFF2Be8rUOuB04AhhO7s6rI5PnVgceAlZK1o8HTgHqgXeAEYCAG4A7k30OA/6QPJ5A7gaFALXAwOQc09rl2BUYnxynBrgT2AH4D2Aq0A8YALwG/Hw572MV/jWB4/eB3y4nx+XAmHavmQiMSB5vQ24a8iX73QnUJuunAY8BfZLvxQdAr+S5VZOvfYFpwGrtv6/L+T7vA1yRPO6dfP/6Ak3AScn2PsBTwDpp///hxYuXri/FNrua2Wf1lfRc8vhhcvfsWRN4KyImJ9tHkusaeTR3Wx96k7vtwQbAPyLiVYDkJmhNyznHzsB3IXdXXuATSasss8+uyfJssr4yucKnP3BrRHyanOOODt7HUGCCpDWSfP/I96aTFqBtgRuT9wS5wmCJG5OsS9wVEQuBhZJmA43AdOA/Je2T7DMsyfxBnlPfA/xeUh9yN/x7KCIWSNoV2FTSmGS/gcmx8r4PM8seFyhm3WNBRGzefkPyC3t++03k7v9y4DL7bQ501z0nBPy/iPjzMuf4aYHn+G/gvIi4Q9JO5Fo98qkBPl72vbczf5n1he0etwJ1yXm+Cnw5Ij6VNIlcq1KHIqI52W834ADguuQpAT+OiPs6yW1mGecxKGblMxn4iqT1ACT1k/QFcnfUXUfSusl+B3bw+onkuo5IxloMAOaSax1Z4j7ge+3GtqwlqYFc19I+kvpK6g98o4NzDAT+mTw+tIN9lp4zIuYA/5C0X3I+Sdqsw+9Ax+f8KClONiDX0rTEYkm9Onjd9cDhwPbk3jfJ1yOWvEbSFySt1MU8ZpYBLlDMyiQi3iM3luM65e6oOxnYICKayXXp3JUMkn2rg0P8BBglaSrwNLBRRHxArstomqRzI+J+4Frg8WS/m4D+EfEMuTEszwE3k+uGWp7TyHXXPAy838E+1wPHJQNp1wUOBsZKeh54Adi7wG/JEveSa0mZAvyK3PdlifHAlCWDZJdxP7nxNX+JiEXJtouBvwPPSJoG/Bm3FJtVJN/N2MzMzDLHLShmZmaWOS5QzMzMLHNcoJiZmVnmuEAxMzOzzHGBYmZmZpnjAsXMzMwyxwWKmZmZZY4LFDMzM8uc/w8Wt2K2JkdO+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5dcf602b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another option is to use the heatmap method of the seaborn package to \n",
    "# visualize the confusion matrix. This generates more elegant ilustrations.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import seaborn and matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "# Generate a new figure and set the figure size.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Generate a heatmap for the confusion matrix using seaborn\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, linewidth=0.5, cmap='PuBu')\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted alternative')\n",
    "ax.set_ylabel('True alternative')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Figures can be stored as images by means of ax.figure.savefig() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown, the confusion matrix gives lot of useful information, but sometimes it is preferred a more concise metric. An other interesting way of assessing the performance of a model is to look the accuracy of the positive predictions, which is called the precision. \n",
    "\n",
    "This metric has an important drawback because if the model makes a single positive prediction and it is correct, then the precision will be 1/1 = 100\\%. This results would not be useful since the classifier would ignore all the predictions except for the only one positive instance. For that reason, the precision metric is commonly used along with another metric called recall, or sensitivity. The recall is the ratio of positive instances that are correctly detected by the classifier. \n",
    "\n",
    "Finally, there is a third metric called F-score, which combines the precision and the recall results to compute the score. The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. The $\\beta$ parameter determines the weight of recall in the combined score. When $\\beta < 1$, the F-score lends more weight to precision, while $\\beta > 1$ favors recall. Moreover, $\\beta \\rightarrow 0$ considers only the precision and $\\beta \\rightarrow \\infty$ only the recall.\n",
    "\n",
    "This metrics can be applied directly to problems where only two alternatives are considered. For problems with more than two alternatives an averaging technique should be applied. PyKernelLogit offers two averaging techniques:\n",
    "* Macro-averaging.\n",
    "* Micro-averaging. \n",
    "\n",
    "The precision, recall and F-score metrics can be computed for the previous model using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6470588235294118, 0.6470588235294118, 0.6470588235294118)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the precision, recall and F1-score for the linear model using a\n",
    "# Micro-averaging technique. The F1-score is the F-score for beta = 1.\n",
    "linear_model.precision_recall_fscore(test_set, \"micro\", beta = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Prediction using the estimated model\n",
    "\n",
    "Once the model has been estimated, new instances can be predicted using the model. The next block of code shows how to create a pandas dataframe which contains a matrix with the probabilities predicted for each alternative for a given observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.772521</td>\n",
       "      <td>0.106371</td>\n",
       "      <td>0.023409</td>\n",
       "      <td>0.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.755860</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.051430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.980807</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.722584</td>\n",
       "      <td>0.146762</td>\n",
       "      <td>0.121924</td>\n",
       "      <td>0.008730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.117608</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.760232</td>\n",
       "      <td>0.116938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.459840</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.237586</td>\n",
       "      <td>0.252489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.054879</td>\n",
       "      <td>0.144570</td>\n",
       "      <td>0.079071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.835638</td>\n",
       "      <td>0.150721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.918013</td>\n",
       "      <td>0.058907</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.005622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.679929</td>\n",
       "      <td>0.088273</td>\n",
       "      <td>0.201389</td>\n",
       "      <td>0.030408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "0    0.772521  0.106371  0.023409  0.097700\n",
       "1    0.755860  0.183258  0.009452  0.051430\n",
       "2    0.980807  0.015501  0.001293  0.002400\n",
       "3    0.722584  0.146762  0.121924  0.008730\n",
       "4    0.117608  0.005222  0.760232  0.116938\n",
       "..        ...       ...       ...       ...\n",
       "131  0.459840  0.050085  0.237586  0.252489\n",
       "132  0.721479  0.054879  0.144570  0.079071\n",
       "133  0.006314  0.007327  0.835638  0.150721\n",
       "134  0.918013  0.058907  0.017458  0.005622\n",
       "135  0.679929  0.088273  0.201389  0.030408\n",
       "\n",
       "[136 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The different observations from the test_set are represented on each row.\n",
    "# The columns represent each of the alternatives. The value from each cell\n",
    "# is the probability that the alternative on the corresponding column was\n",
    "# selected on that observation.\n",
    "pd.DataFrame(data=linear_model.predict(test_set).reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Computation of the Willingness to Pay (WTP) indicator using the estimated model\n",
    "\n",
    "There are many indicators that are particularly relevant in the context of discrete choice models, such as, Willingness to Pay (WTP) and the Value of Time (VOT). If the estimated model contains a cost or price variable, which indicates the total cost of the alternative for a given decision-maker, it is possible to analyze the trade-off between any other variable form the model and money. This trade-off reflects the **willingness of the decision-maker to pay** for a modification of one unit of another variable of the model.\n",
    "\n",
    "Lets consider that $c_{in}$ is the cost of the alternative $i$ for a decision-maker $n$, and $x_{in}$ is the value of another variable. Now, let $V_{in}(c_{in}, x_{in})$ be the value of the utility function that is associated to an alternative $i$ for a decision-maker $n$. It can be considered a scenario where the variable under interest takes the value $x_{in} + \\delta^{x}_{in}$, where $\\delta^{x}_{in}$ is an increment over the value of the $x$ variable. It is denoted by $\\delta^{c}_{in}$ the additional cost increment that would be necessary to achieve the same utility as if no increment were made over $x$ variable, that is\n",
    "\\begin{equation}\n",
    "    \\label{eq:WTP-same-utility}\n",
    "    V_{i n}\\left(c_{i n}+\\delta_{i n}^{c}, x_{i n}+\\delta_{i n}^{x}\\right)=V_{i n}\\left(c_{i n}, x_{i n}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "If $x_{in}$ and $c_{in}$ appear linearly in the utility function, that is if \n",
    "$V_{i n}\\left(c_{i n}, x_{i n}\\right)=\\beta_{c} c_{i n}+\\beta_{x} x_{i n}+\\cdots$\n",
    "Then, the WTP can be calculated directly using the coefficients ratio as:\n",
    "\\begin{equation}\n",
    "    WTP = \\frac{\\delta_{i n}^{c}}{\\delta_{i n}^{x}}=-\\frac{\\left(\\partial V_{i n} / \\partial x_{i n}\\right)\\left(c_{i n}, x_{i n}\\right)}{\\left(\\partial V_{i n} / \\partial c_{i n}\\right)\\left(c_{i n}, x_{i n}\\right)}=-\\frac{\\beta_{x}}{\\beta_{c}}\n",
    "\\end{equation}\n",
    "where $\\beta_{c}$ is the calculated coefficient for the cost.\n",
    "\n",
    "The above equation is the willingness to pay for an increase of the value of\n",
    "the variable $x_{in}$. Therefore, if this increase improves the utility of the alternative, then $\\beta_{x}>0$.\n",
    "\n",
    "\n",
    "Another concept is the **Value of Time (VOT)**, which is closely related with the previous one, as it reflects the price that a traveler is willing to pay to decrease the travel time in one unit. Hence, the formula for the VOT is:\n",
    "\\begin{equation}\n",
    "    \\mathrm{VOT}_{i n}=\\delta_{i n}^{c} /\\left(-\\delta_{i n}^{t}\\right)=\\frac{\\left(\\partial V_{i n} / \\partial t_{i n}\\right)\\left(c_{i n}, t_{i n}\\right)}{\\left(\\partial V_{i n} / \\partial c_{i n}\\right)\\left(c_{i n}, t_{i n}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "As $V$ is linear in the cost and time parameters, we have\n",
    "\\begin{equation}\n",
    "    \\mathrm{VOT}_{i n}=\\delta_{i n}^{c} /\\left(-\\delta_{i n}^{t}\\right)=\\frac{\\beta_{t}}{\\beta_{c}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it is interesting to compute the **VOT** as the model contains the cost and time variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.60497059316263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compute the VOT for the linear model, we need to apply the previous equation.\n",
    "# Take into account that travel time is in minutes, therefore beta_t = 1/minutes,\n",
    "# which is equal as beta_t = 60 * 1/hours. Now, VOT is measured in dollars/hours.\n",
    "\n",
    "(linear_model.coefs['Travel time (minutes)']*60) / linear_model.coefs['Travel cost (dollars)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimating a non-parametric model using Kernel Logistic Regression (KLR)\n",
    "\n",
    "Now, a non-parametric model is estimated using KLR capabilities from PyKernelLogit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Definition of a KLR model\n",
    "\n",
    "The first step is to consider if the data should be preprocessed. It is a good practice to scale the data before a KLR model is applied. Skcikit-learn Python package can be used to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(train_set[['cost', 'time']])\n",
    "\n",
    "train_set_scaled = train_set.copy()\n",
    "train_set_scaled[['cost', 'time']] = scaler.transform(train_set[['cost', 'time']])\n",
    "\n",
    "test_set_scaled = test_set.copy()\n",
    "test_set_scaled[['cost', 'time']] = scaler.transform(test_set[['cost', 'time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step to define a KLR model is to compute the kernel matrix, or Gram matrix, using the input dataframe. The kernel_matrix function returns a list composed of one kernel matrix $\\mathbf{K}^{(i)}$ per alternative $i$. This function takes the following input values:\n",
    "* X. The dataset in long format for which the kernel matrix is computed, i.e. $\\mathbf{X}_{i}$. \n",
    "\n",
    "* alt_id_col. The name for the alternative ID column in the long dataframe.\n",
    "\n",
    "* obs_id_col. The column from the wide dataset that contains the ID of each observation.\n",
    "\n",
    "* variables. A Python dictionary where each key represents each of the alternatives IDs and its associated value contains a list of characteristics from the long dataframe X to be used on that alternative.\n",
    "\n",
    "* kernel_type. The type of kernel function to be used. The kernel_matrix function relies on the scikit-learn Python package for the kernel functions. The value passed on the kernel_type parameter specifies the scikit-learn kernel function which is going to be used to compute the kernel matrix.\n",
    "\n",
    "* Z. The kernel_matrix function also allows the user to specify a reference matrix $\\mathbf{Z}_{i}$ to be used when computing the kernel matrix. If no Z dataframe is specified, then the kernel matrix is computed taking as reference matrix the input matrix X.\n",
    "\n",
    "* \\*\\*kernel\\_kwargs. Keyworded variable length of arguments to be provided to the invoked scikit-learn kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {1: ['cost', 'time'],\n",
    "             2: ['cost', 'time'],\n",
    "             3: ['cost', 'time'],\n",
    "             4: ['cost', 'time']}\n",
    "\n",
    "\n",
    "\n",
    "K_long_format_train = pkl.long_format_with_kernel_matrix(train_set_scaled, \n",
    "                                                         custom_alt_id, \n",
    "                                                         obs_id_column, \n",
    "                                                         choice_column, \n",
    "                                                         variables, \n",
    "                                                         kernel_type = \"RBF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the KLR model specification. Similarly to the linear model specification, it is necessary to create two ordered dictionaries:\n",
    "* Specification ordered dictionary.\n",
    "* Variable names ordered dictionary.\n",
    "\n",
    "The PyKernelLogit function define_kernel_specification generates automatically the specification and variable names ordered dictionaries for the desired kernel model using the specification attributes provided through its input parameters:\n",
    "* K_long_format. A long format dataframe that contains the kernel matrix for all the alternatives in the dataset. This dataframe should be obtained using the long_format_with_kernel_matrix function.\n",
    "\n",
    "* alt_id_col. The name for the alternative ID column in the long dataframe.\n",
    "\n",
    "* obs_id_col. The column from the wide dataset that contains the ID of each observation.\n",
    "\n",
    "* specification. A previously defined specification ordered dictionary where the kernel logistic model specification will be inserted. It is recommended to pass an empty ordered dictionary.\n",
    "\n",
    "* names. A previously defined variable names ordered dictionary where it will be inserted the names for the $\\boldsymbol{\\alpha}_i$ parameters to be estimated. It is recommended to pass an empty ordered dictionary.\n",
    "\n",
    "* alpha_per_alternative. A boolean variable. If this this variable is True then one vector of parameters to estimate $\\boldsymbol{\\alpha}_i$ is used per alternative $i$. Otherwise, the same vector of parameters $\\boldsymbol{\\alpha}$ is used for all the alternatives. This generates a more restricted model, but this model will have much less parameters to be estimated. The default value is False.\n",
    "\n",
    "* intercept. If this parameter is None (the default value), then no intercept parameter is added to the model. If this parameter contains an integer value, then an intercept parameter is added to the model. The intercept parameter must be the alternative ID of the alternative selected to be the reference alternative. No intercept is generated for the reference alternative because it is considered to be zero.\n",
    "\n",
    "This method returns a tuple with 3 elements: the basic_specification and basic_names ordered dictionaries and the total number of variables to be estimated in the model.\n",
    "\n",
    "The following code fragment is used to define the basic_specification and basic_names ordered dictionaries for a KLR model with the same $\\boldsymbol{\\alpha}$ vector of parameters for all the alternatives and an intercept parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "[basic_specification, basic_names, total_vars] = pkl.define_kernel_specification(K_long_format_train, \n",
    "                                                                                 custom_alt_id,\n",
    "                                                                                 obs_id_column,\n",
    "                                                                                 specification=OrderedDict(),\n",
    "                                                                                 names=OrderedDict(),\n",
    "                                                                                 alpha_per_alternative=False,\n",
    "                                                                                 intercept=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Estimation of the previous KLR model\n",
    "\n",
    "Once the KLR model has been specified, it is necessary to estimate its parameters in a similar way that the previous linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the multinomial logit model (MNL)\n",
    "kernel_model = pkl.create_choice_model(data=K_long_format_train,\n",
    "                                       alt_id_col=custom_alt_id,\n",
    "                                       obs_id_col=obs_id_column,\n",
    "                                       choice_col=choice_column,\n",
    "                                       specification=basic_specification,\n",
    "                                       model_type=\"MNL\",\n",
    "                                       names=basic_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous model can be also estimated using the fit_mle() method. Now, instead of using Maximum Likelihood Estimation (MLE), it was decided to use Penalized Maximum Likelihood Estimation (PMLE) due to the high number of parameters. It was decided to use a Ridge regression with a penalty parameter $\\lambda$ = 10. Moreover,  L-BFGS-B optimization method was used to estimate the parameters because, although it obtains slightly worse results than BFGS method, the parameters are estimated remarkably fast as it is designed to optimize a large number of parameters using a limited amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -439.4553\n",
      "Initial Log-likelihood: -439.4553\n",
      "Estimation Time for Point Estimation: 0.05 seconds.\n",
      "Final log-likelihood: -275.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseangel/.local/lib/python3.6/site-packages/PyKernelLogit/conditional_logit.py:376: UserWarning: NOTE: A penalized regression is being performed. The reported standard errors and robust standard errors ***WILL BE INCORRECT***.\n",
      "  warnings.warn(_ridge_warning_msg)\n",
      "/home/joseangel/.local/lib/python3.6/site-packages/PyKernelLogit/base_multinomial_cm_v2.py:1230: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self._store_inferential_results(np.sqrt(np.diag(self.cov)),\n",
      "/home/joseangel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/home/joseangel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/home/joseangel/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/home/joseangel/.local/lib/python3.6/site-packages/PyKernelLogit/base_multinomial_cm_v2.py:1261: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self._store_inferential_results(np.sqrt(np.diag(self.robust_cov)),\n"
     ]
    }
   ],
   "source": [
    "# Specify the initial values and method for the optimization.\n",
    "kernel_model.fit_mle(np.zeros(total_vars), method=\"L-BFGS-B\", PMLE=\"RIDGE\", PMLE_lambda=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluation of the estimated model\n",
    "\n",
    "Once the model has been estimated, the performance of the model can be assessed such as in the linear model. Therefore, get_statsmodels_summary() can be used to obtain the goodness of fit of the model and the estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multinomial Logit Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>choice</td>          <th>  No. Observations:  </th>    <td>317</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>         <td>Multinomial Logit Model</td> <th>  Df Residuals:      </th>    <td>-3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>           <th>  Df Model:          </th>    <td>320</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Oct 2019</td>     <th>  Pseudo R-squ.:     </th>   <td>0.374</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:01:36</td>         <th>  Pseudo R-bar-squ.: </th>  <td>-0.354</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AIC:</th>                  <td>1,190.009</td>        <th>  Log-Likelihood:    </th> <td>-275.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BIC:</th>                  <td>2,392.858</td>        <th>  LL-Null:           </th> <td>-439.455</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC 2</th>     <td>   -0.8490</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC 3</th>     <td>   -0.4187</td> <td>  321.129</td> <td>   -0.001</td> <td> 0.999</td> <td> -629.820</td> <td>  628.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ASC 4</th>     <td>   -0.0071</td> <td>  558.393</td> <td>-1.27e-05</td> <td> 1.000</td> <td>-1094.437</td> <td> 1094.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_1</th>   <td>    0.1136</td> <td> 3.23e+06</td> <td> 3.51e-08</td> <td> 1.000</td> <td>-6.34e+06</td> <td> 6.34e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_2</th>   <td>    0.1642</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_3</th>   <td>    0.0165</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_4</th>   <td>   -0.1592</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_5</th>   <td>   -0.0292</td> <td> 7.22e+06</td> <td>-4.04e-09</td> <td> 1.000</td> <td>-1.41e+07</td> <td> 1.41e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_6</th>   <td>    0.0663</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_7</th>   <td>    0.0520</td> <td> 5.54e+06</td> <td>  9.4e-09</td> <td> 1.000</td> <td>-1.09e+07</td> <td> 1.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_8</th>   <td>   -0.0296</td> <td> 1.35e+07</td> <td> -2.2e-09</td> <td> 1.000</td> <td>-2.64e+07</td> <td> 2.64e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_9</th>   <td>   -0.1621</td> <td> 5.03e+06</td> <td>-3.22e-08</td> <td> 1.000</td> <td>-9.85e+06</td> <td> 9.85e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_10</th>  <td>   -0.2157</td> <td> 9.94e+06</td> <td>-2.17e-08</td> <td> 1.000</td> <td>-1.95e+07</td> <td> 1.95e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_11</th>  <td>    0.1200</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_12</th>  <td>    0.0497</td> <td> 3.65e+06</td> <td> 1.36e-08</td> <td> 1.000</td> <td>-7.15e+06</td> <td> 7.15e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_13</th>  <td>    0.1976</td> <td> 5.06e+06</td> <td>  3.9e-08</td> <td> 1.000</td> <td>-9.92e+06</td> <td> 9.92e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_14</th>  <td>    0.0086</td> <td>  4.2e+06</td> <td> 2.05e-09</td> <td> 1.000</td> <td>-8.23e+06</td> <td> 8.23e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_15</th>  <td>   -0.1049</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_16</th>  <td>    0.0787</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_17</th>  <td>   -0.0626</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_18</th>  <td>   -0.1943</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_19</th>  <td>   -0.0651</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_20</th>  <td>   -0.1075</td> <td> 4.17e+06</td> <td>-2.58e-08</td> <td> 1.000</td> <td>-8.18e+06</td> <td> 8.18e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_21</th>  <td>    0.0682</td> <td> 3.32e+06</td> <td> 2.05e-08</td> <td> 1.000</td> <td>-6.51e+06</td> <td> 6.51e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_22</th>  <td>   -0.0050</td> <td> 3.47e+06</td> <td>-1.45e-09</td> <td> 1.000</td> <td> -6.8e+06</td> <td>  6.8e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_23</th>  <td>    0.0858</td> <td> 8.35e+06</td> <td> 1.03e-08</td> <td> 1.000</td> <td>-1.64e+07</td> <td> 1.64e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_24</th>  <td>    0.0507</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_25</th>  <td>    0.2280</td> <td> 4.15e+06</td> <td>  5.5e-08</td> <td> 1.000</td> <td>-8.13e+06</td> <td> 8.13e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_26</th>  <td>   -0.1442</td> <td> 5.42e+06</td> <td>-2.66e-08</td> <td> 1.000</td> <td>-1.06e+07</td> <td> 1.06e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_27</th>  <td>    0.1653</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_28</th>  <td>    0.0135</td> <td> 3.77e+06</td> <td> 3.58e-09</td> <td> 1.000</td> <td> -7.4e+06</td> <td>  7.4e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_29</th>  <td>    0.1017</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_30</th>  <td>   -0.0192</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_31</th>  <td>    0.1142</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_32</th>  <td>   -0.0174</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_33</th>  <td>   -0.2510</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_34</th>  <td>   -0.0208</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_35</th>  <td>    0.0253</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_36</th>  <td>   -0.0939</td> <td> 1.53e+06</td> <td>-6.15e-08</td> <td> 1.000</td> <td>-2.99e+06</td> <td> 2.99e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_37</th>  <td>   -0.0034</td> <td> 2.58e+06</td> <td> -1.3e-09</td> <td> 1.000</td> <td>-5.05e+06</td> <td> 5.05e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_38</th>  <td>   -0.0315</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_39</th>  <td>    0.0895</td> <td> 7.21e+06</td> <td> 1.24e-08</td> <td> 1.000</td> <td>-1.41e+07</td> <td> 1.41e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_40</th>  <td>    0.0823</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_41</th>  <td>   -0.0226</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_42</th>  <td>   -0.1213</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_43</th>  <td>    0.1807</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_44</th>  <td>   -0.2424</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_45</th>  <td>   -0.3074</td> <td> 9.73e+06</td> <td>-3.16e-08</td> <td> 1.000</td> <td>-1.91e+07</td> <td> 1.91e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_46</th>  <td>    0.0024</td> <td>  1.8e+06</td> <td> 1.35e-09</td> <td> 1.000</td> <td>-3.53e+06</td> <td> 3.53e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_47</th>  <td>    0.1005</td> <td>  3.5e+06</td> <td> 2.87e-08</td> <td> 1.000</td> <td>-6.86e+06</td> <td> 6.86e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_48</th>  <td>   -0.0626</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_49</th>  <td>    0.0098</td> <td> 3.94e+06</td> <td> 2.48e-09</td> <td> 1.000</td> <td>-7.73e+06</td> <td> 7.73e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_50</th>  <td>   -0.2400</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_51</th>  <td>    0.0647</td> <td> 4.56e+06</td> <td> 1.42e-08</td> <td> 1.000</td> <td>-8.93e+06</td> <td> 8.93e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_52</th>  <td>   -0.0803</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_53</th>  <td>    0.1642</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_54</th>  <td>   -0.1065</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_55</th>  <td>    0.1649</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_56</th>  <td>   -0.3106</td> <td> 7.37e+06</td> <td>-4.22e-08</td> <td> 1.000</td> <td>-1.44e+07</td> <td> 1.44e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_57</th>  <td>   -0.1901</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_58</th>  <td>   -0.0386</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_59</th>  <td>    0.1393</td> <td> 3.48e+06</td> <td> 4.01e-08</td> <td> 1.000</td> <td>-6.81e+06</td> <td> 6.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_60</th>  <td>    0.0255</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_61</th>  <td>   -0.1738</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_62</th>  <td>   -0.1198</td> <td> 1.69e+06</td> <td>-7.07e-08</td> <td> 1.000</td> <td>-3.32e+06</td> <td> 3.32e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_63</th>  <td>   -0.0809</td> <td> 5.04e+06</td> <td>-1.61e-08</td> <td> 1.000</td> <td>-9.87e+06</td> <td> 9.87e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_64</th>  <td>   -0.0464</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_65</th>  <td>    0.0201</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_66</th>  <td>   -0.0748</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_67</th>  <td>    0.1926</td> <td> 2.91e+06</td> <td> 6.63e-08</td> <td> 1.000</td> <td> -5.7e+06</td> <td>  5.7e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_68</th>  <td>    0.0428</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_69</th>  <td>   -0.1520</td> <td> 9.68e+05</td> <td>-1.57e-07</td> <td> 1.000</td> <td> -1.9e+06</td> <td>  1.9e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_70</th>  <td>   -0.0858</td> <td> 6.41e+06</td> <td>-1.34e-08</td> <td> 1.000</td> <td>-1.26e+07</td> <td> 1.26e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_71</th>  <td>    0.0296</td> <td> 4.86e+06</td> <td> 6.09e-09</td> <td> 1.000</td> <td>-9.53e+06</td> <td> 9.53e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_72</th>  <td>    0.0782</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_73</th>  <td>    0.0744</td> <td> 5.09e+06</td> <td> 1.46e-08</td> <td> 1.000</td> <td>-9.98e+06</td> <td> 9.98e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_74</th>  <td>   -0.1872</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_75</th>  <td>    0.1936</td> <td> 8.01e+05</td> <td> 2.42e-07</td> <td> 1.000</td> <td>-1.57e+06</td> <td> 1.57e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_76</th>  <td>    0.0776</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_77</th>  <td>   -0.0914</td> <td> 1.02e+07</td> <td>-8.98e-09</td> <td> 1.000</td> <td>-1.99e+07</td> <td> 1.99e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_78</th>  <td>   -0.0007</td> <td> 4.32e+06</td> <td>-1.56e-10</td> <td> 1.000</td> <td>-8.46e+06</td> <td> 8.46e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_79</th>  <td>    0.1687</td> <td> 5.93e+05</td> <td> 2.84e-07</td> <td> 1.000</td> <td>-1.16e+06</td> <td> 1.16e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_80</th>  <td>    0.1284</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_81</th>  <td>   -0.0417</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_82</th>  <td>   -0.0130</td> <td>  5.9e+06</td> <td>-2.21e-09</td> <td> 1.000</td> <td>-1.16e+07</td> <td> 1.16e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_83</th>  <td>   -0.0327</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_84</th>  <td>   -0.0798</td> <td> 2.96e+06</td> <td> -2.7e-08</td> <td> 1.000</td> <td> -5.8e+06</td> <td>  5.8e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_85</th>  <td>    0.0665</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_86</th>  <td>    0.1586</td> <td> 5.57e+06</td> <td> 2.85e-08</td> <td> 1.000</td> <td>-1.09e+07</td> <td> 1.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_87</th>  <td>   -0.0584</td> <td> 7.01e+06</td> <td>-8.34e-09</td> <td> 1.000</td> <td>-1.37e+07</td> <td> 1.37e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_88</th>  <td>   -0.1174</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_89</th>  <td>   -0.1488</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_90</th>  <td>    0.0514</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_91</th>  <td>    0.1732</td> <td> 5.01e+06</td> <td> 3.46e-08</td> <td> 1.000</td> <td>-9.82e+06</td> <td> 9.82e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_92</th>  <td>   -0.0972</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_93</th>  <td>   -0.0774</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_94</th>  <td>    0.0404</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_95</th>  <td>    0.0169</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_96</th>  <td>   -0.0666</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_97</th>  <td>   -0.1966</td> <td> 3.51e+06</td> <td> -5.6e-08</td> <td> 1.000</td> <td>-6.88e+06</td> <td> 6.88e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_98</th>  <td>    0.1046</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_99</th>  <td>    0.1123</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_100</th> <td>    0.0090</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_101</th> <td>   -0.0740</td> <td> 4.41e+06</td> <td>-1.68e-08</td> <td> 1.000</td> <td>-8.64e+06</td> <td> 8.64e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_102</th> <td>    0.0731</td> <td>  9.1e+06</td> <td> 8.04e-09</td> <td> 1.000</td> <td>-1.78e+07</td> <td> 1.78e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_103</th> <td>   -0.1415</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_104</th> <td>   -0.2987</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_105</th> <td>    0.1695</td> <td> 5.57e+06</td> <td> 3.04e-08</td> <td> 1.000</td> <td>-1.09e+07</td> <td> 1.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_106</th> <td>   -0.1457</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_107</th> <td>   -0.0210</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_108</th> <td>   -0.1225</td> <td> 3.66e+06</td> <td>-3.35e-08</td> <td> 1.000</td> <td>-7.17e+06</td> <td> 7.17e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_109</th> <td>   -0.1487</td> <td> 5.02e+06</td> <td>-2.96e-08</td> <td> 1.000</td> <td>-9.85e+06</td> <td> 9.85e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_110</th> <td>    0.0443</td> <td> 4.15e+06</td> <td> 1.07e-08</td> <td> 1.000</td> <td>-8.13e+06</td> <td> 8.13e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_111</th> <td>   -0.2926</td> <td> 3.74e+06</td> <td>-7.82e-08</td> <td> 1.000</td> <td>-7.34e+06</td> <td> 7.34e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_112</th> <td>   -0.0103</td> <td> 4.46e+06</td> <td> -2.3e-09</td> <td> 1.000</td> <td>-8.73e+06</td> <td> 8.73e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_113</th> <td>   -0.2238</td> <td> 2.36e+06</td> <td>-9.47e-08</td> <td> 1.000</td> <td>-4.63e+06</td> <td> 4.63e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_114</th> <td>   -0.1339</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_115</th> <td>   -0.0432</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_116</th> <td>   -0.1467</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_117</th> <td>   -0.0072</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_118</th> <td>    0.0920</td> <td> 4.78e+06</td> <td> 1.92e-08</td> <td> 1.000</td> <td>-9.37e+06</td> <td> 9.37e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_119</th> <td>   -0.1030</td> <td> 5.37e+06</td> <td>-1.92e-08</td> <td> 1.000</td> <td>-1.05e+07</td> <td> 1.05e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_120</th> <td>   -0.0948</td> <td>    2e+06</td> <td>-4.73e-08</td> <td> 1.000</td> <td>-3.92e+06</td> <td> 3.92e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_121</th> <td>   -0.3004</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_122</th> <td>    0.1786</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_123</th> <td>   -0.0658</td> <td> 1.67e+06</td> <td>-3.95e-08</td> <td> 1.000</td> <td>-3.27e+06</td> <td> 3.27e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_124</th> <td>   -0.0780</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_125</th> <td>   -0.0412</td> <td> 3.17e+06</td> <td> -1.3e-08</td> <td> 1.000</td> <td>-6.21e+06</td> <td> 6.21e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_126</th> <td>    0.0478</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_127</th> <td>    0.0691</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_128</th> <td>    0.0325</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_129</th> <td>    0.0767</td> <td> 1.92e+06</td> <td>    4e-08</td> <td> 1.000</td> <td>-3.76e+06</td> <td> 3.76e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_130</th> <td>    0.1562</td> <td> 1.99e+06</td> <td> 7.83e-08</td> <td> 1.000</td> <td>-3.91e+06</td> <td> 3.91e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_131</th> <td>    0.0049</td> <td> 6.41e+06</td> <td> 7.67e-10</td> <td> 1.000</td> <td>-1.26e+07</td> <td> 1.26e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_132</th> <td>    0.0637</td> <td> 8.32e+06</td> <td> 7.66e-09</td> <td> 1.000</td> <td>-1.63e+07</td> <td> 1.63e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_133</th> <td>   -0.3517</td> <td> 8.29e+06</td> <td>-4.24e-08</td> <td> 1.000</td> <td>-1.62e+07</td> <td> 1.62e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_134</th> <td>   -0.2130</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_135</th> <td>    0.1784</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_136</th> <td>   -0.1465</td> <td> 3.25e+06</td> <td>-4.51e-08</td> <td> 1.000</td> <td>-6.36e+06</td> <td> 6.36e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_137</th> <td>    0.1411</td> <td> 6.82e+06</td> <td> 2.07e-08</td> <td> 1.000</td> <td>-1.34e+07</td> <td> 1.34e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_138</th> <td>    0.0200</td> <td> 5.07e+06</td> <td> 3.94e-09</td> <td> 1.000</td> <td>-9.93e+06</td> <td> 9.93e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_139</th> <td>   -0.2164</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_140</th> <td>    0.1744</td> <td> 1.41e+06</td> <td> 1.24e-07</td> <td> 1.000</td> <td>-2.76e+06</td> <td> 2.76e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_141</th> <td>    0.1121</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_142</th> <td>    0.1406</td> <td> 1.52e+06</td> <td> 9.25e-08</td> <td> 1.000</td> <td>-2.98e+06</td> <td> 2.98e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_143</th> <td>    0.1660</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_144</th> <td>    0.0224</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_145</th> <td>   -0.1436</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_146</th> <td>    0.2074</td> <td> 2.25e+06</td> <td> 9.21e-08</td> <td> 1.000</td> <td>-4.41e+06</td> <td> 4.41e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_147</th> <td>   -0.0606</td> <td> 5.06e+06</td> <td> -1.2e-08</td> <td> 1.000</td> <td>-9.91e+06</td> <td> 9.91e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_148</th> <td>    0.0099</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_149</th> <td>   -0.0880</td> <td> 5.59e+06</td> <td>-1.58e-08</td> <td> 1.000</td> <td>-1.09e+07</td> <td> 1.09e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_150</th> <td>   -0.1300</td> <td> 1.04e+07</td> <td>-1.24e-08</td> <td> 1.000</td> <td>-2.05e+07</td> <td> 2.05e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_151</th> <td>   -0.0593</td> <td>  1.7e+06</td> <td> -3.5e-08</td> <td> 1.000</td> <td>-3.33e+06</td> <td> 3.33e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_152</th> <td>   -0.0325</td> <td> 1.23e+07</td> <td>-2.64e-09</td> <td> 1.000</td> <td>-2.41e+07</td> <td> 2.41e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_153</th> <td>   -0.0722</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_154</th> <td>   -0.0425</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_155</th> <td>    0.0408</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_156</th> <td>    0.0110</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_157</th> <td>    0.0209</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_158</th> <td>   -0.1405</td> <td> 4.03e+06</td> <td>-3.49e-08</td> <td> 1.000</td> <td>-7.89e+06</td> <td> 7.89e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_159</th> <td>    0.0403</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_160</th> <td>   -0.1527</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_161</th> <td>    0.0992</td> <td> 3.47e+06</td> <td> 2.86e-08</td> <td> 1.000</td> <td> -6.8e+06</td> <td>  6.8e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_162</th> <td>   -0.0624</td> <td> 5.45e+06</td> <td>-1.15e-08</td> <td> 1.000</td> <td>-1.07e+07</td> <td> 1.07e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_163</th> <td>   -0.0386</td> <td> 4.51e+06</td> <td>-8.56e-09</td> <td> 1.000</td> <td>-8.85e+06</td> <td> 8.85e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_164</th> <td>   -0.1036</td> <td> 2.62e+06</td> <td>-3.96e-08</td> <td> 1.000</td> <td>-5.13e+06</td> <td> 5.13e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_165</th> <td>    0.0850</td> <td> 1.78e+06</td> <td> 4.77e-08</td> <td> 1.000</td> <td>-3.49e+06</td> <td> 3.49e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_166</th> <td>   -0.1480</td> <td> 6.08e+06</td> <td>-2.43e-08</td> <td> 1.000</td> <td>-1.19e+07</td> <td> 1.19e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_167</th> <td>    0.0602</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_168</th> <td>   -0.0902</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_169</th> <td>    0.0091</td> <td> 6.79e+06</td> <td> 1.34e-09</td> <td> 1.000</td> <td>-1.33e+07</td> <td> 1.33e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_170</th> <td>   -0.1485</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_171</th> <td>    0.0278</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_172</th> <td>   -0.2168</td> <td> 9.62e+05</td> <td>-2.25e-07</td> <td> 1.000</td> <td>-1.89e+06</td> <td> 1.89e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_173</th> <td>    0.0245</td> <td> 2.12e+06</td> <td> 1.16e-08</td> <td> 1.000</td> <td>-4.15e+06</td> <td> 4.15e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_174</th> <td>    0.1321</td> <td>  4.9e+06</td> <td>  2.7e-08</td> <td> 1.000</td> <td>-9.59e+06</td> <td> 9.59e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_175</th> <td>    0.0877</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_176</th> <td>   -0.0618</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_177</th> <td>    0.0590</td> <td> 4.42e+06</td> <td> 1.33e-08</td> <td> 1.000</td> <td>-8.67e+06</td> <td> 8.67e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_178</th> <td>   -0.0316</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_179</th> <td>    0.1907</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_180</th> <td>    0.0818</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_181</th> <td>   -0.2255</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_182</th> <td>    0.0458</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_183</th> <td>   -0.0131</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_184</th> <td>    0.0041</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_185</th> <td>   -0.3106</td> <td> 2.67e+06</td> <td>-1.16e-07</td> <td> 1.000</td> <td>-5.23e+06</td> <td> 5.23e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_186</th> <td>    0.0288</td> <td> 1.95e+06</td> <td> 1.48e-08</td> <td> 1.000</td> <td>-3.83e+06</td> <td> 3.83e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_187</th> <td>    0.0200</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_188</th> <td>   -0.0184</td> <td> 4.41e+06</td> <td>-4.17e-09</td> <td> 1.000</td> <td>-8.64e+06</td> <td> 8.64e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_189</th> <td>    0.1832</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_190</th> <td>   -0.1816</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_191</th> <td>   -0.0408</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_192</th> <td>    0.0634</td> <td> 6.62e+06</td> <td> 9.57e-09</td> <td> 1.000</td> <td> -1.3e+07</td> <td>  1.3e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_193</th> <td>   -0.0198</td> <td> 3.02e+06</td> <td>-6.56e-09</td> <td> 1.000</td> <td>-5.92e+06</td> <td> 5.92e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_194</th> <td>    0.0897</td> <td> 1.62e+06</td> <td> 5.52e-08</td> <td> 1.000</td> <td>-3.18e+06</td> <td> 3.18e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_195</th> <td>    0.0395</td> <td> 1.82e+06</td> <td> 2.17e-08</td> <td> 1.000</td> <td>-3.57e+06</td> <td> 3.57e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_196</th> <td>   -0.0395</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_197</th> <td>   -0.0678</td> <td> 4.18e+06</td> <td>-1.62e-08</td> <td> 1.000</td> <td>-8.19e+06</td> <td> 8.19e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_198</th> <td>   -0.1309</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_199</th> <td>   -0.1680</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_200</th> <td>    0.0129</td> <td> 4.15e+06</td> <td> 3.11e-09</td> <td> 1.000</td> <td>-8.14e+06</td> <td> 8.14e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_201</th> <td>   -0.2107</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_202</th> <td>   -0.1769</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_203</th> <td>   -0.0325</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_204</th> <td>   -0.0937</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_205</th> <td>    0.0074</td> <td> 4.54e+06</td> <td> 1.64e-09</td> <td> 1.000</td> <td>-8.91e+06</td> <td> 8.91e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_206</th> <td>   -0.0690</td> <td> 1.43e+06</td> <td>-4.82e-08</td> <td> 1.000</td> <td>-2.81e+06</td> <td> 2.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_207</th> <td>   -0.0124</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_208</th> <td>    0.0944</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_209</th> <td>   -0.2771</td> <td> 4.81e+06</td> <td>-5.76e-08</td> <td> 1.000</td> <td>-9.43e+06</td> <td> 9.43e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_210</th> <td>    0.1388</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_211</th> <td>   -0.0346</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_212</th> <td>   -0.2191</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_213</th> <td>    0.0083</td> <td> 3.98e+06</td> <td> 2.08e-09</td> <td> 1.000</td> <td>-7.79e+06</td> <td> 7.79e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_214</th> <td>   -0.0112</td> <td> 5.49e+06</td> <td>-2.03e-09</td> <td> 1.000</td> <td>-1.08e+07</td> <td> 1.08e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_215</th> <td>   -0.0676</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_216</th> <td>    0.0380</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_217</th> <td>   -0.0473</td> <td> 2.97e+06</td> <td>-1.59e-08</td> <td> 1.000</td> <td>-5.82e+06</td> <td> 5.82e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_218</th> <td>    0.1820</td> <td> 1.81e+06</td> <td> 1.01e-07</td> <td> 1.000</td> <td>-3.55e+06</td> <td> 3.55e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_219</th> <td>    0.0236</td> <td> 2.71e+06</td> <td> 8.69e-09</td> <td> 1.000</td> <td>-5.31e+06</td> <td> 5.31e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_220</th> <td>   -0.1346</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_221</th> <td>   -0.0048</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_222</th> <td>   -0.0178</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_223</th> <td>    0.0823</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_224</th> <td>    0.0545</td> <td> 1.28e+06</td> <td> 4.25e-08</td> <td> 1.000</td> <td>-2.52e+06</td> <td> 2.52e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_225</th> <td>   -0.1692</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_226</th> <td>   -0.0217</td> <td> 4.49e+06</td> <td>-4.83e-09</td> <td> 1.000</td> <td>-8.81e+06</td> <td> 8.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_227</th> <td>   -0.1356</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_228</th> <td>   -0.0477</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_229</th> <td>   -0.1420</td> <td> 5.04e+06</td> <td>-2.82e-08</td> <td> 1.000</td> <td>-9.87e+06</td> <td> 9.87e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_230</th> <td>   -0.1478</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_231</th> <td>    0.0461</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_232</th> <td>    0.0276</td> <td> 2.53e+06</td> <td> 1.09e-08</td> <td> 1.000</td> <td>-4.96e+06</td> <td> 4.96e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_233</th> <td>   -0.1347</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_234</th> <td>   -0.0496</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_235</th> <td>   -0.1386</td> <td> 3.23e+06</td> <td>-4.29e-08</td> <td> 1.000</td> <td>-6.34e+06</td> <td> 6.34e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_236</th> <td>   -0.1765</td> <td> 4.91e+06</td> <td>-3.59e-08</td> <td> 1.000</td> <td>-9.63e+06</td> <td> 9.63e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_237</th> <td>    0.0058</td> <td> 4.59e+06</td> <td> 1.27e-09</td> <td> 1.000</td> <td>   -9e+06</td> <td>    9e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_238</th> <td>   -0.1854</td> <td> 7.87e+05</td> <td>-2.35e-07</td> <td> 1.000</td> <td>-1.54e+06</td> <td> 1.54e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_239</th> <td>   -0.0232</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_240</th> <td>   -0.1674</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_241</th> <td>   -0.2764</td> <td> 9.71e+05</td> <td>-2.85e-07</td> <td> 1.000</td> <td> -1.9e+06</td> <td>  1.9e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_242</th> <td>    0.0055</td> <td> 1.66e+06</td> <td>  3.3e-09</td> <td> 1.000</td> <td>-3.24e+06</td> <td> 3.24e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_243</th> <td>   -0.0510</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_244</th> <td>    0.0366</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_245</th> <td>    0.0198</td> <td> 2.44e+06</td> <td> 8.11e-09</td> <td> 1.000</td> <td>-4.78e+06</td> <td> 4.78e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_246</th> <td>   -0.0029</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_247</th> <td>    0.0408</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_248</th> <td>    0.0547</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_249</th> <td>    0.0318</td> <td> 2.26e+06</td> <td> 1.41e-08</td> <td> 1.000</td> <td>-4.42e+06</td> <td> 4.42e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_250</th> <td>    0.0349</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_251</th> <td>   -0.0942</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_252</th> <td>   -0.0055</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_253</th> <td>   -0.0482</td> <td> 3.03e+05</td> <td>-1.59e-07</td> <td> 1.000</td> <td>-5.94e+05</td> <td> 5.94e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_254</th> <td>    0.2270</td> <td>  4.2e+06</td> <td>  5.4e-08</td> <td> 1.000</td> <td>-8.24e+06</td> <td> 8.24e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_255</th> <td>   -0.1255</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_256</th> <td>    0.1120</td> <td> 2.48e+06</td> <td> 4.52e-08</td> <td> 1.000</td> <td>-4.86e+06</td> <td> 4.86e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_257</th> <td>   -0.0522</td> <td> 3.86e+06</td> <td>-1.35e-08</td> <td> 1.000</td> <td>-7.57e+06</td> <td> 7.57e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_258</th> <td>    0.0087</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_259</th> <td>   -0.0855</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_260</th> <td>   -0.0097</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_261</th> <td>    0.0889</td> <td> 1.92e+06</td> <td> 4.62e-08</td> <td> 1.000</td> <td>-3.77e+06</td> <td> 3.77e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_262</th> <td>    0.0949</td> <td> 2.12e+06</td> <td> 4.47e-08</td> <td> 1.000</td> <td>-4.16e+06</td> <td> 4.16e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_263</th> <td>    0.1259</td> <td> 3.61e+05</td> <td> 3.48e-07</td> <td> 1.000</td> <td>-7.08e+05</td> <td> 7.08e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_264</th> <td>   -0.1134</td> <td> 3.12e+06</td> <td>-3.64e-08</td> <td> 1.000</td> <td>-6.11e+06</td> <td> 6.11e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_265</th> <td>   -0.1387</td> <td>  2.2e+06</td> <td> -6.3e-08</td> <td> 1.000</td> <td>-4.31e+06</td> <td> 4.31e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_266</th> <td>    0.1766</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_267</th> <td>   -0.0755</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_268</th> <td>    0.0178</td> <td> 2.51e+06</td> <td> 7.07e-09</td> <td> 1.000</td> <td>-4.93e+06</td> <td> 4.93e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_269</th> <td>    0.0666</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_270</th> <td>   -0.0598</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_271</th> <td>    0.0674</td> <td> 9.86e+05</td> <td> 6.83e-08</td> <td> 1.000</td> <td>-1.93e+06</td> <td> 1.93e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_272</th> <td>    0.0356</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_273</th> <td>   -0.1260</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_274</th> <td>    0.1016</td> <td>  5.2e+06</td> <td> 1.95e-08</td> <td> 1.000</td> <td>-1.02e+07</td> <td> 1.02e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_275</th> <td>   -0.1179</td> <td> 2.11e+06</td> <td>-5.58e-08</td> <td> 1.000</td> <td>-4.14e+06</td> <td> 4.14e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_276</th> <td>    0.0825</td> <td> 3.83e+06</td> <td> 2.15e-08</td> <td> 1.000</td> <td>-7.51e+06</td> <td> 7.51e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_277</th> <td>   -0.2035</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_278</th> <td>   -0.1987</td> <td> 3.36e+06</td> <td>-5.91e-08</td> <td> 1.000</td> <td>-6.59e+06</td> <td> 6.59e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_279</th> <td>   -0.2416</td> <td> 5.16e+06</td> <td>-4.68e-08</td> <td> 1.000</td> <td>-1.01e+07</td> <td> 1.01e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_280</th> <td>    0.1182</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_281</th> <td>   -0.0201</td> <td> 2.24e+06</td> <td>-8.99e-09</td> <td> 1.000</td> <td>-4.39e+06</td> <td> 4.39e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_282</th> <td>    0.0595</td> <td> 2.16e+06</td> <td> 2.75e-08</td> <td> 1.000</td> <td>-4.24e+06</td> <td> 4.24e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_283</th> <td>   -0.2489</td> <td> 3.85e+06</td> <td>-6.47e-08</td> <td> 1.000</td> <td>-7.54e+06</td> <td> 7.54e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_284</th> <td>    0.0407</td> <td> 3.23e+06</td> <td> 1.26e-08</td> <td> 1.000</td> <td>-6.34e+06</td> <td> 6.34e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_285</th> <td>    0.0376</td> <td> 3.36e+06</td> <td> 1.12e-08</td> <td> 1.000</td> <td>-6.59e+06</td> <td> 6.59e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_286</th> <td>   -0.1397</td> <td>    2e+06</td> <td>-6.98e-08</td> <td> 1.000</td> <td>-3.92e+06</td> <td> 3.92e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_287</th> <td>    0.0447</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_288</th> <td>   -0.1238</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_289</th> <td>   -0.1083</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_290</th> <td>    0.1168</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_291</th> <td>    0.1591</td> <td> 2.67e+06</td> <td> 5.95e-08</td> <td> 1.000</td> <td>-5.24e+06</td> <td> 5.24e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_292</th> <td>   -0.2742</td> <td> 4.33e+06</td> <td>-6.34e-08</td> <td> 1.000</td> <td>-8.48e+06</td> <td> 8.48e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_293</th> <td>    0.0115</td> <td> 4.31e+06</td> <td> 2.66e-09</td> <td> 1.000</td> <td>-8.45e+06</td> <td> 8.45e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_294</th> <td>    0.0410</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_295</th> <td>   -0.1103</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_296</th> <td>   -0.2553</td> <td> 1.19e+06</td> <td>-2.15e-07</td> <td> 1.000</td> <td>-2.32e+06</td> <td> 2.32e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_297</th> <td>    0.0488</td> <td> 1.94e+06</td> <td> 2.51e-08</td> <td> 1.000</td> <td>-3.81e+06</td> <td> 3.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_298</th> <td>   -0.0412</td> <td> 2.28e+06</td> <td>-1.81e-08</td> <td> 1.000</td> <td>-4.46e+06</td> <td> 4.46e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_299</th> <td>    0.0228</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_300</th> <td>   -0.1721</td> <td> 3.34e+06</td> <td>-5.16e-08</td> <td> 1.000</td> <td>-6.54e+06</td> <td> 6.54e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_301</th> <td>   -0.0757</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_302</th> <td>    0.0911</td> <td> 2.94e+06</td> <td>  3.1e-08</td> <td> 1.000</td> <td>-5.77e+06</td> <td> 5.77e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_303</th> <td>   -0.0747</td> <td> 2.28e+06</td> <td>-3.27e-08</td> <td> 1.000</td> <td>-4.48e+06</td> <td> 4.48e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_304</th> <td>    0.0906</td> <td> 2.85e+06</td> <td> 3.18e-08</td> <td> 1.000</td> <td>-5.59e+06</td> <td> 5.59e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_305</th> <td>    0.0358</td> <td> 3.62e+06</td> <td> 9.89e-09</td> <td> 1.000</td> <td>-7.09e+06</td> <td> 7.09e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_306</th> <td>    0.1583</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_307</th> <td>   -0.1472</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_308</th> <td>    0.0603</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_309</th> <td>   -0.0288</td> <td>  1.3e+06</td> <td>-2.21e-08</td> <td> 1.000</td> <td>-2.55e+06</td> <td> 2.55e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_310</th> <td>    0.0060</td> <td> 3.99e+06</td> <td>  1.5e-09</td> <td> 1.000</td> <td>-7.82e+06</td> <td> 7.82e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_311</th> <td>    0.0429</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_312</th> <td>    0.0898</td> <td> 1.32e+06</td> <td>  6.8e-08</td> <td> 1.000</td> <td>-2.59e+06</td> <td> 2.59e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_313</th> <td>   -0.0332</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_314</th> <td>   -0.0151</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_315</th> <td>    0.1044</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_316</th> <td>   -0.2449</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha_317</th> <td>   -0.2731</td> <td>  2.7e+06</td> <td>-1.01e-07</td> <td> 1.000</td> <td>-5.29e+06</td> <td> 5.29e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Multinomial Logit Model Regression Results                    \n",
       "===================================================================================\n",
       "Dep. Variable:                      choice   No. Observations:                  317\n",
       "Model:             Multinomial Logit Model   Df Residuals:                       -3\n",
       "Method:                                MLE   Df Model:                          320\n",
       "Date:                     Wed, 02 Oct 2019   Pseudo R-squ.:                   0.374\n",
       "Time:                             20:01:36   Pseudo R-bar-squ.:              -0.354\n",
       "AIC:                             1,190.009   Log-Likelihood:               -275.005\n",
       "BIC:                             2,392.858   LL-Null:                      -439.455\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "ASC 2         -0.8490        nan        nan        nan         nan         nan\n",
       "ASC 3         -0.4187    321.129     -0.001      0.999    -629.820     628.983\n",
       "ASC 4         -0.0071    558.393  -1.27e-05      1.000   -1094.437    1094.422\n",
       "alpha_1        0.1136   3.23e+06   3.51e-08      1.000   -6.34e+06    6.34e+06\n",
       "alpha_2        0.1642        nan        nan        nan         nan         nan\n",
       "alpha_3        0.0165        nan        nan        nan         nan         nan\n",
       "alpha_4       -0.1592        nan        nan        nan         nan         nan\n",
       "alpha_5       -0.0292   7.22e+06  -4.04e-09      1.000   -1.41e+07    1.41e+07\n",
       "alpha_6        0.0663        nan        nan        nan         nan         nan\n",
       "alpha_7        0.0520   5.54e+06    9.4e-09      1.000   -1.09e+07    1.09e+07\n",
       "alpha_8       -0.0296   1.35e+07   -2.2e-09      1.000   -2.64e+07    2.64e+07\n",
       "alpha_9       -0.1621   5.03e+06  -3.22e-08      1.000   -9.85e+06    9.85e+06\n",
       "alpha_10      -0.2157   9.94e+06  -2.17e-08      1.000   -1.95e+07    1.95e+07\n",
       "alpha_11       0.1200        nan        nan        nan         nan         nan\n",
       "alpha_12       0.0497   3.65e+06   1.36e-08      1.000   -7.15e+06    7.15e+06\n",
       "alpha_13       0.1976   5.06e+06    3.9e-08      1.000   -9.92e+06    9.92e+06\n",
       "alpha_14       0.0086    4.2e+06   2.05e-09      1.000   -8.23e+06    8.23e+06\n",
       "alpha_15      -0.1049        nan        nan        nan         nan         nan\n",
       "alpha_16       0.0787        nan        nan        nan         nan         nan\n",
       "alpha_17      -0.0626        nan        nan        nan         nan         nan\n",
       "alpha_18      -0.1943        nan        nan        nan         nan         nan\n",
       "alpha_19      -0.0651        nan        nan        nan         nan         nan\n",
       "alpha_20      -0.1075   4.17e+06  -2.58e-08      1.000   -8.18e+06    8.18e+06\n",
       "alpha_21       0.0682   3.32e+06   2.05e-08      1.000   -6.51e+06    6.51e+06\n",
       "alpha_22      -0.0050   3.47e+06  -1.45e-09      1.000    -6.8e+06     6.8e+06\n",
       "alpha_23       0.0858   8.35e+06   1.03e-08      1.000   -1.64e+07    1.64e+07\n",
       "alpha_24       0.0507        nan        nan        nan         nan         nan\n",
       "alpha_25       0.2280   4.15e+06    5.5e-08      1.000   -8.13e+06    8.13e+06\n",
       "alpha_26      -0.1442   5.42e+06  -2.66e-08      1.000   -1.06e+07    1.06e+07\n",
       "alpha_27       0.1653        nan        nan        nan         nan         nan\n",
       "alpha_28       0.0135   3.77e+06   3.58e-09      1.000    -7.4e+06     7.4e+06\n",
       "alpha_29       0.1017        nan        nan        nan         nan         nan\n",
       "alpha_30      -0.0192        nan        nan        nan         nan         nan\n",
       "alpha_31       0.1142        nan        nan        nan         nan         nan\n",
       "alpha_32      -0.0174        nan        nan        nan         nan         nan\n",
       "alpha_33      -0.2510        nan        nan        nan         nan         nan\n",
       "alpha_34      -0.0208        nan        nan        nan         nan         nan\n",
       "alpha_35       0.0253        nan        nan        nan         nan         nan\n",
       "alpha_36      -0.0939   1.53e+06  -6.15e-08      1.000   -2.99e+06    2.99e+06\n",
       "alpha_37      -0.0034   2.58e+06   -1.3e-09      1.000   -5.05e+06    5.05e+06\n",
       "alpha_38      -0.0315        nan        nan        nan         nan         nan\n",
       "alpha_39       0.0895   7.21e+06   1.24e-08      1.000   -1.41e+07    1.41e+07\n",
       "alpha_40       0.0823        nan        nan        nan         nan         nan\n",
       "alpha_41      -0.0226        nan        nan        nan         nan         nan\n",
       "alpha_42      -0.1213        nan        nan        nan         nan         nan\n",
       "alpha_43       0.1807        nan        nan        nan         nan         nan\n",
       "alpha_44      -0.2424        nan        nan        nan         nan         nan\n",
       "alpha_45      -0.3074   9.73e+06  -3.16e-08      1.000   -1.91e+07    1.91e+07\n",
       "alpha_46       0.0024    1.8e+06   1.35e-09      1.000   -3.53e+06    3.53e+06\n",
       "alpha_47       0.1005    3.5e+06   2.87e-08      1.000   -6.86e+06    6.86e+06\n",
       "alpha_48      -0.0626        nan        nan        nan         nan         nan\n",
       "alpha_49       0.0098   3.94e+06   2.48e-09      1.000   -7.73e+06    7.73e+06\n",
       "alpha_50      -0.2400        nan        nan        nan         nan         nan\n",
       "alpha_51       0.0647   4.56e+06   1.42e-08      1.000   -8.93e+06    8.93e+06\n",
       "alpha_52      -0.0803        nan        nan        nan         nan         nan\n",
       "alpha_53       0.1642        nan        nan        nan         nan         nan\n",
       "alpha_54      -0.1065        nan        nan        nan         nan         nan\n",
       "alpha_55       0.1649        nan        nan        nan         nan         nan\n",
       "alpha_56      -0.3106   7.37e+06  -4.22e-08      1.000   -1.44e+07    1.44e+07\n",
       "alpha_57      -0.1901        nan        nan        nan         nan         nan\n",
       "alpha_58      -0.0386        nan        nan        nan         nan         nan\n",
       "alpha_59       0.1393   3.48e+06   4.01e-08      1.000   -6.81e+06    6.81e+06\n",
       "alpha_60       0.0255        nan        nan        nan         nan         nan\n",
       "alpha_61      -0.1738        nan        nan        nan         nan         nan\n",
       "alpha_62      -0.1198   1.69e+06  -7.07e-08      1.000   -3.32e+06    3.32e+06\n",
       "alpha_63      -0.0809   5.04e+06  -1.61e-08      1.000   -9.87e+06    9.87e+06\n",
       "alpha_64      -0.0464        nan        nan        nan         nan         nan\n",
       "alpha_65       0.0201        nan        nan        nan         nan         nan\n",
       "alpha_66      -0.0748        nan        nan        nan         nan         nan\n",
       "alpha_67       0.1926   2.91e+06   6.63e-08      1.000    -5.7e+06     5.7e+06\n",
       "alpha_68       0.0428        nan        nan        nan         nan         nan\n",
       "alpha_69      -0.1520   9.68e+05  -1.57e-07      1.000    -1.9e+06     1.9e+06\n",
       "alpha_70      -0.0858   6.41e+06  -1.34e-08      1.000   -1.26e+07    1.26e+07\n",
       "alpha_71       0.0296   4.86e+06   6.09e-09      1.000   -9.53e+06    9.53e+06\n",
       "alpha_72       0.0782        nan        nan        nan         nan         nan\n",
       "alpha_73       0.0744   5.09e+06   1.46e-08      1.000   -9.98e+06    9.98e+06\n",
       "alpha_74      -0.1872        nan        nan        nan         nan         nan\n",
       "alpha_75       0.1936   8.01e+05   2.42e-07      1.000   -1.57e+06    1.57e+06\n",
       "alpha_76       0.0776        nan        nan        nan         nan         nan\n",
       "alpha_77      -0.0914   1.02e+07  -8.98e-09      1.000   -1.99e+07    1.99e+07\n",
       "alpha_78      -0.0007   4.32e+06  -1.56e-10      1.000   -8.46e+06    8.46e+06\n",
       "alpha_79       0.1687   5.93e+05   2.84e-07      1.000   -1.16e+06    1.16e+06\n",
       "alpha_80       0.1284        nan        nan        nan         nan         nan\n",
       "alpha_81      -0.0417        nan        nan        nan         nan         nan\n",
       "alpha_82      -0.0130    5.9e+06  -2.21e-09      1.000   -1.16e+07    1.16e+07\n",
       "alpha_83      -0.0327        nan        nan        nan         nan         nan\n",
       "alpha_84      -0.0798   2.96e+06   -2.7e-08      1.000    -5.8e+06     5.8e+06\n",
       "alpha_85       0.0665        nan        nan        nan         nan         nan\n",
       "alpha_86       0.1586   5.57e+06   2.85e-08      1.000   -1.09e+07    1.09e+07\n",
       "alpha_87      -0.0584   7.01e+06  -8.34e-09      1.000   -1.37e+07    1.37e+07\n",
       "alpha_88      -0.1174        nan        nan        nan         nan         nan\n",
       "alpha_89      -0.1488        nan        nan        nan         nan         nan\n",
       "alpha_90       0.0514        nan        nan        nan         nan         nan\n",
       "alpha_91       0.1732   5.01e+06   3.46e-08      1.000   -9.82e+06    9.82e+06\n",
       "alpha_92      -0.0972        nan        nan        nan         nan         nan\n",
       "alpha_93      -0.0774        nan        nan        nan         nan         nan\n",
       "alpha_94       0.0404        nan        nan        nan         nan         nan\n",
       "alpha_95       0.0169        nan        nan        nan         nan         nan\n",
       "alpha_96      -0.0666        nan        nan        nan         nan         nan\n",
       "alpha_97      -0.1966   3.51e+06   -5.6e-08      1.000   -6.88e+06    6.88e+06\n",
       "alpha_98       0.1046        nan        nan        nan         nan         nan\n",
       "alpha_99       0.1123        nan        nan        nan         nan         nan\n",
       "alpha_100      0.0090        nan        nan        nan         nan         nan\n",
       "alpha_101     -0.0740   4.41e+06  -1.68e-08      1.000   -8.64e+06    8.64e+06\n",
       "alpha_102      0.0731    9.1e+06   8.04e-09      1.000   -1.78e+07    1.78e+07\n",
       "alpha_103     -0.1415        nan        nan        nan         nan         nan\n",
       "alpha_104     -0.2987        nan        nan        nan         nan         nan\n",
       "alpha_105      0.1695   5.57e+06   3.04e-08      1.000   -1.09e+07    1.09e+07\n",
       "alpha_106     -0.1457        nan        nan        nan         nan         nan\n",
       "alpha_107     -0.0210        nan        nan        nan         nan         nan\n",
       "alpha_108     -0.1225   3.66e+06  -3.35e-08      1.000   -7.17e+06    7.17e+06\n",
       "alpha_109     -0.1487   5.02e+06  -2.96e-08      1.000   -9.85e+06    9.85e+06\n",
       "alpha_110      0.0443   4.15e+06   1.07e-08      1.000   -8.13e+06    8.13e+06\n",
       "alpha_111     -0.2926   3.74e+06  -7.82e-08      1.000   -7.34e+06    7.34e+06\n",
       "alpha_112     -0.0103   4.46e+06   -2.3e-09      1.000   -8.73e+06    8.73e+06\n",
       "alpha_113     -0.2238   2.36e+06  -9.47e-08      1.000   -4.63e+06    4.63e+06\n",
       "alpha_114     -0.1339        nan        nan        nan         nan         nan\n",
       "alpha_115     -0.0432        nan        nan        nan         nan         nan\n",
       "alpha_116     -0.1467        nan        nan        nan         nan         nan\n",
       "alpha_117     -0.0072        nan        nan        nan         nan         nan\n",
       "alpha_118      0.0920   4.78e+06   1.92e-08      1.000   -9.37e+06    9.37e+06\n",
       "alpha_119     -0.1030   5.37e+06  -1.92e-08      1.000   -1.05e+07    1.05e+07\n",
       "alpha_120     -0.0948      2e+06  -4.73e-08      1.000   -3.92e+06    3.92e+06\n",
       "alpha_121     -0.3004        nan        nan        nan         nan         nan\n",
       "alpha_122      0.1786        nan        nan        nan         nan         nan\n",
       "alpha_123     -0.0658   1.67e+06  -3.95e-08      1.000   -3.27e+06    3.27e+06\n",
       "alpha_124     -0.0780        nan        nan        nan         nan         nan\n",
       "alpha_125     -0.0412   3.17e+06   -1.3e-08      1.000   -6.21e+06    6.21e+06\n",
       "alpha_126      0.0478        nan        nan        nan         nan         nan\n",
       "alpha_127      0.0691        nan        nan        nan         nan         nan\n",
       "alpha_128      0.0325        nan        nan        nan         nan         nan\n",
       "alpha_129      0.0767   1.92e+06      4e-08      1.000   -3.76e+06    3.76e+06\n",
       "alpha_130      0.1562   1.99e+06   7.83e-08      1.000   -3.91e+06    3.91e+06\n",
       "alpha_131      0.0049   6.41e+06   7.67e-10      1.000   -1.26e+07    1.26e+07\n",
       "alpha_132      0.0637   8.32e+06   7.66e-09      1.000   -1.63e+07    1.63e+07\n",
       "alpha_133     -0.3517   8.29e+06  -4.24e-08      1.000   -1.62e+07    1.62e+07\n",
       "alpha_134     -0.2130        nan        nan        nan         nan         nan\n",
       "alpha_135      0.1784        nan        nan        nan         nan         nan\n",
       "alpha_136     -0.1465   3.25e+06  -4.51e-08      1.000   -6.36e+06    6.36e+06\n",
       "alpha_137      0.1411   6.82e+06   2.07e-08      1.000   -1.34e+07    1.34e+07\n",
       "alpha_138      0.0200   5.07e+06   3.94e-09      1.000   -9.93e+06    9.93e+06\n",
       "alpha_139     -0.2164        nan        nan        nan         nan         nan\n",
       "alpha_140      0.1744   1.41e+06   1.24e-07      1.000   -2.76e+06    2.76e+06\n",
       "alpha_141      0.1121        nan        nan        nan         nan         nan\n",
       "alpha_142      0.1406   1.52e+06   9.25e-08      1.000   -2.98e+06    2.98e+06\n",
       "alpha_143      0.1660        nan        nan        nan         nan         nan\n",
       "alpha_144      0.0224        nan        nan        nan         nan         nan\n",
       "alpha_145     -0.1436        nan        nan        nan         nan         nan\n",
       "alpha_146      0.2074   2.25e+06   9.21e-08      1.000   -4.41e+06    4.41e+06\n",
       "alpha_147     -0.0606   5.06e+06   -1.2e-08      1.000   -9.91e+06    9.91e+06\n",
       "alpha_148      0.0099        nan        nan        nan         nan         nan\n",
       "alpha_149     -0.0880   5.59e+06  -1.58e-08      1.000   -1.09e+07    1.09e+07\n",
       "alpha_150     -0.1300   1.04e+07  -1.24e-08      1.000   -2.05e+07    2.05e+07\n",
       "alpha_151     -0.0593    1.7e+06   -3.5e-08      1.000   -3.33e+06    3.33e+06\n",
       "alpha_152     -0.0325   1.23e+07  -2.64e-09      1.000   -2.41e+07    2.41e+07\n",
       "alpha_153     -0.0722        nan        nan        nan         nan         nan\n",
       "alpha_154     -0.0425        nan        nan        nan         nan         nan\n",
       "alpha_155      0.0408        nan        nan        nan         nan         nan\n",
       "alpha_156      0.0110        nan        nan        nan         nan         nan\n",
       "alpha_157      0.0209        nan        nan        nan         nan         nan\n",
       "alpha_158     -0.1405   4.03e+06  -3.49e-08      1.000   -7.89e+06    7.89e+06\n",
       "alpha_159      0.0403        nan        nan        nan         nan         nan\n",
       "alpha_160     -0.1527        nan        nan        nan         nan         nan\n",
       "alpha_161      0.0992   3.47e+06   2.86e-08      1.000    -6.8e+06     6.8e+06\n",
       "alpha_162     -0.0624   5.45e+06  -1.15e-08      1.000   -1.07e+07    1.07e+07\n",
       "alpha_163     -0.0386   4.51e+06  -8.56e-09      1.000   -8.85e+06    8.85e+06\n",
       "alpha_164     -0.1036   2.62e+06  -3.96e-08      1.000   -5.13e+06    5.13e+06\n",
       "alpha_165      0.0850   1.78e+06   4.77e-08      1.000   -3.49e+06    3.49e+06\n",
       "alpha_166     -0.1480   6.08e+06  -2.43e-08      1.000   -1.19e+07    1.19e+07\n",
       "alpha_167      0.0602        nan        nan        nan         nan         nan\n",
       "alpha_168     -0.0902        nan        nan        nan         nan         nan\n",
       "alpha_169      0.0091   6.79e+06   1.34e-09      1.000   -1.33e+07    1.33e+07\n",
       "alpha_170     -0.1485        nan        nan        nan         nan         nan\n",
       "alpha_171      0.0278        nan        nan        nan         nan         nan\n",
       "alpha_172     -0.2168   9.62e+05  -2.25e-07      1.000   -1.89e+06    1.89e+06\n",
       "alpha_173      0.0245   2.12e+06   1.16e-08      1.000   -4.15e+06    4.15e+06\n",
       "alpha_174      0.1321    4.9e+06    2.7e-08      1.000   -9.59e+06    9.59e+06\n",
       "alpha_175      0.0877        nan        nan        nan         nan         nan\n",
       "alpha_176     -0.0618        nan        nan        nan         nan         nan\n",
       "alpha_177      0.0590   4.42e+06   1.33e-08      1.000   -8.67e+06    8.67e+06\n",
       "alpha_178     -0.0316        nan        nan        nan         nan         nan\n",
       "alpha_179      0.1907        nan        nan        nan         nan         nan\n",
       "alpha_180      0.0818        nan        nan        nan         nan         nan\n",
       "alpha_181     -0.2255        nan        nan        nan         nan         nan\n",
       "alpha_182      0.0458        nan        nan        nan         nan         nan\n",
       "alpha_183     -0.0131        nan        nan        nan         nan         nan\n",
       "alpha_184      0.0041        nan        nan        nan         nan         nan\n",
       "alpha_185     -0.3106   2.67e+06  -1.16e-07      1.000   -5.23e+06    5.23e+06\n",
       "alpha_186      0.0288   1.95e+06   1.48e-08      1.000   -3.83e+06    3.83e+06\n",
       "alpha_187      0.0200        nan        nan        nan         nan         nan\n",
       "alpha_188     -0.0184   4.41e+06  -4.17e-09      1.000   -8.64e+06    8.64e+06\n",
       "alpha_189      0.1832        nan        nan        nan         nan         nan\n",
       "alpha_190     -0.1816        nan        nan        nan         nan         nan\n",
       "alpha_191     -0.0408        nan        nan        nan         nan         nan\n",
       "alpha_192      0.0634   6.62e+06   9.57e-09      1.000    -1.3e+07     1.3e+07\n",
       "alpha_193     -0.0198   3.02e+06  -6.56e-09      1.000   -5.92e+06    5.92e+06\n",
       "alpha_194      0.0897   1.62e+06   5.52e-08      1.000   -3.18e+06    3.18e+06\n",
       "alpha_195      0.0395   1.82e+06   2.17e-08      1.000   -3.57e+06    3.57e+06\n",
       "alpha_196     -0.0395        nan        nan        nan         nan         nan\n",
       "alpha_197     -0.0678   4.18e+06  -1.62e-08      1.000   -8.19e+06    8.19e+06\n",
       "alpha_198     -0.1309        nan        nan        nan         nan         nan\n",
       "alpha_199     -0.1680        nan        nan        nan         nan         nan\n",
       "alpha_200      0.0129   4.15e+06   3.11e-09      1.000   -8.14e+06    8.14e+06\n",
       "alpha_201     -0.2107        nan        nan        nan         nan         nan\n",
       "alpha_202     -0.1769        nan        nan        nan         nan         nan\n",
       "alpha_203     -0.0325        nan        nan        nan         nan         nan\n",
       "alpha_204     -0.0937        nan        nan        nan         nan         nan\n",
       "alpha_205      0.0074   4.54e+06   1.64e-09      1.000   -8.91e+06    8.91e+06\n",
       "alpha_206     -0.0690   1.43e+06  -4.82e-08      1.000   -2.81e+06    2.81e+06\n",
       "alpha_207     -0.0124        nan        nan        nan         nan         nan\n",
       "alpha_208      0.0944        nan        nan        nan         nan         nan\n",
       "alpha_209     -0.2771   4.81e+06  -5.76e-08      1.000   -9.43e+06    9.43e+06\n",
       "alpha_210      0.1388        nan        nan        nan         nan         nan\n",
       "alpha_211     -0.0346        nan        nan        nan         nan         nan\n",
       "alpha_212     -0.2191        nan        nan        nan         nan         nan\n",
       "alpha_213      0.0083   3.98e+06   2.08e-09      1.000   -7.79e+06    7.79e+06\n",
       "alpha_214     -0.0112   5.49e+06  -2.03e-09      1.000   -1.08e+07    1.08e+07\n",
       "alpha_215     -0.0676        nan        nan        nan         nan         nan\n",
       "alpha_216      0.0380        nan        nan        nan         nan         nan\n",
       "alpha_217     -0.0473   2.97e+06  -1.59e-08      1.000   -5.82e+06    5.82e+06\n",
       "alpha_218      0.1820   1.81e+06   1.01e-07      1.000   -3.55e+06    3.55e+06\n",
       "alpha_219      0.0236   2.71e+06   8.69e-09      1.000   -5.31e+06    5.31e+06\n",
       "alpha_220     -0.1346        nan        nan        nan         nan         nan\n",
       "alpha_221     -0.0048        nan        nan        nan         nan         nan\n",
       "alpha_222     -0.0178        nan        nan        nan         nan         nan\n",
       "alpha_223      0.0823        nan        nan        nan         nan         nan\n",
       "alpha_224      0.0545   1.28e+06   4.25e-08      1.000   -2.52e+06    2.52e+06\n",
       "alpha_225     -0.1692        nan        nan        nan         nan         nan\n",
       "alpha_226     -0.0217   4.49e+06  -4.83e-09      1.000   -8.81e+06    8.81e+06\n",
       "alpha_227     -0.1356        nan        nan        nan         nan         nan\n",
       "alpha_228     -0.0477        nan        nan        nan         nan         nan\n",
       "alpha_229     -0.1420   5.04e+06  -2.82e-08      1.000   -9.87e+06    9.87e+06\n",
       "alpha_230     -0.1478        nan        nan        nan         nan         nan\n",
       "alpha_231      0.0461        nan        nan        nan         nan         nan\n",
       "alpha_232      0.0276   2.53e+06   1.09e-08      1.000   -4.96e+06    4.96e+06\n",
       "alpha_233     -0.1347        nan        nan        nan         nan         nan\n",
       "alpha_234     -0.0496        nan        nan        nan         nan         nan\n",
       "alpha_235     -0.1386   3.23e+06  -4.29e-08      1.000   -6.34e+06    6.34e+06\n",
       "alpha_236     -0.1765   4.91e+06  -3.59e-08      1.000   -9.63e+06    9.63e+06\n",
       "alpha_237      0.0058   4.59e+06   1.27e-09      1.000      -9e+06       9e+06\n",
       "alpha_238     -0.1854   7.87e+05  -2.35e-07      1.000   -1.54e+06    1.54e+06\n",
       "alpha_239     -0.0232        nan        nan        nan         nan         nan\n",
       "alpha_240     -0.1674        nan        nan        nan         nan         nan\n",
       "alpha_241     -0.2764   9.71e+05  -2.85e-07      1.000    -1.9e+06     1.9e+06\n",
       "alpha_242      0.0055   1.66e+06    3.3e-09      1.000   -3.24e+06    3.24e+06\n",
       "alpha_243     -0.0510        nan        nan        nan         nan         nan\n",
       "alpha_244      0.0366        nan        nan        nan         nan         nan\n",
       "alpha_245      0.0198   2.44e+06   8.11e-09      1.000   -4.78e+06    4.78e+06\n",
       "alpha_246     -0.0029        nan        nan        nan         nan         nan\n",
       "alpha_247      0.0408        nan        nan        nan         nan         nan\n",
       "alpha_248      0.0547        nan        nan        nan         nan         nan\n",
       "alpha_249      0.0318   2.26e+06   1.41e-08      1.000   -4.42e+06    4.42e+06\n",
       "alpha_250      0.0349        nan        nan        nan         nan         nan\n",
       "alpha_251     -0.0942        nan        nan        nan         nan         nan\n",
       "alpha_252     -0.0055        nan        nan        nan         nan         nan\n",
       "alpha_253     -0.0482   3.03e+05  -1.59e-07      1.000   -5.94e+05    5.94e+05\n",
       "alpha_254      0.2270    4.2e+06    5.4e-08      1.000   -8.24e+06    8.24e+06\n",
       "alpha_255     -0.1255        nan        nan        nan         nan         nan\n",
       "alpha_256      0.1120   2.48e+06   4.52e-08      1.000   -4.86e+06    4.86e+06\n",
       "alpha_257     -0.0522   3.86e+06  -1.35e-08      1.000   -7.57e+06    7.57e+06\n",
       "alpha_258      0.0087        nan        nan        nan         nan         nan\n",
       "alpha_259     -0.0855        nan        nan        nan         nan         nan\n",
       "alpha_260     -0.0097        nan        nan        nan         nan         nan\n",
       "alpha_261      0.0889   1.92e+06   4.62e-08      1.000   -3.77e+06    3.77e+06\n",
       "alpha_262      0.0949   2.12e+06   4.47e-08      1.000   -4.16e+06    4.16e+06\n",
       "alpha_263      0.1259   3.61e+05   3.48e-07      1.000   -7.08e+05    7.08e+05\n",
       "alpha_264     -0.1134   3.12e+06  -3.64e-08      1.000   -6.11e+06    6.11e+06\n",
       "alpha_265     -0.1387    2.2e+06   -6.3e-08      1.000   -4.31e+06    4.31e+06\n",
       "alpha_266      0.1766        nan        nan        nan         nan         nan\n",
       "alpha_267     -0.0755        nan        nan        nan         nan         nan\n",
       "alpha_268      0.0178   2.51e+06   7.07e-09      1.000   -4.93e+06    4.93e+06\n",
       "alpha_269      0.0666        nan        nan        nan         nan         nan\n",
       "alpha_270     -0.0598        nan        nan        nan         nan         nan\n",
       "alpha_271      0.0674   9.86e+05   6.83e-08      1.000   -1.93e+06    1.93e+06\n",
       "alpha_272      0.0356        nan        nan        nan         nan         nan\n",
       "alpha_273     -0.1260        nan        nan        nan         nan         nan\n",
       "alpha_274      0.1016    5.2e+06   1.95e-08      1.000   -1.02e+07    1.02e+07\n",
       "alpha_275     -0.1179   2.11e+06  -5.58e-08      1.000   -4.14e+06    4.14e+06\n",
       "alpha_276      0.0825   3.83e+06   2.15e-08      1.000   -7.51e+06    7.51e+06\n",
       "alpha_277     -0.2035        nan        nan        nan         nan         nan\n",
       "alpha_278     -0.1987   3.36e+06  -5.91e-08      1.000   -6.59e+06    6.59e+06\n",
       "alpha_279     -0.2416   5.16e+06  -4.68e-08      1.000   -1.01e+07    1.01e+07\n",
       "alpha_280      0.1182        nan        nan        nan         nan         nan\n",
       "alpha_281     -0.0201   2.24e+06  -8.99e-09      1.000   -4.39e+06    4.39e+06\n",
       "alpha_282      0.0595   2.16e+06   2.75e-08      1.000   -4.24e+06    4.24e+06\n",
       "alpha_283     -0.2489   3.85e+06  -6.47e-08      1.000   -7.54e+06    7.54e+06\n",
       "alpha_284      0.0407   3.23e+06   1.26e-08      1.000   -6.34e+06    6.34e+06\n",
       "alpha_285      0.0376   3.36e+06   1.12e-08      1.000   -6.59e+06    6.59e+06\n",
       "alpha_286     -0.1397      2e+06  -6.98e-08      1.000   -3.92e+06    3.92e+06\n",
       "alpha_287      0.0447        nan        nan        nan         nan         nan\n",
       "alpha_288     -0.1238        nan        nan        nan         nan         nan\n",
       "alpha_289     -0.1083        nan        nan        nan         nan         nan\n",
       "alpha_290      0.1168        nan        nan        nan         nan         nan\n",
       "alpha_291      0.1591   2.67e+06   5.95e-08      1.000   -5.24e+06    5.24e+06\n",
       "alpha_292     -0.2742   4.33e+06  -6.34e-08      1.000   -8.48e+06    8.48e+06\n",
       "alpha_293      0.0115   4.31e+06   2.66e-09      1.000   -8.45e+06    8.45e+06\n",
       "alpha_294      0.0410        nan        nan        nan         nan         nan\n",
       "alpha_295     -0.1103        nan        nan        nan         nan         nan\n",
       "alpha_296     -0.2553   1.19e+06  -2.15e-07      1.000   -2.32e+06    2.32e+06\n",
       "alpha_297      0.0488   1.94e+06   2.51e-08      1.000   -3.81e+06    3.81e+06\n",
       "alpha_298     -0.0412   2.28e+06  -1.81e-08      1.000   -4.46e+06    4.46e+06\n",
       "alpha_299      0.0228        nan        nan        nan         nan         nan\n",
       "alpha_300     -0.1721   3.34e+06  -5.16e-08      1.000   -6.54e+06    6.54e+06\n",
       "alpha_301     -0.0757        nan        nan        nan         nan         nan\n",
       "alpha_302      0.0911   2.94e+06    3.1e-08      1.000   -5.77e+06    5.77e+06\n",
       "alpha_303     -0.0747   2.28e+06  -3.27e-08      1.000   -4.48e+06    4.48e+06\n",
       "alpha_304      0.0906   2.85e+06   3.18e-08      1.000   -5.59e+06    5.59e+06\n",
       "alpha_305      0.0358   3.62e+06   9.89e-09      1.000   -7.09e+06    7.09e+06\n",
       "alpha_306      0.1583        nan        nan        nan         nan         nan\n",
       "alpha_307     -0.1472        nan        nan        nan         nan         nan\n",
       "alpha_308      0.0603        nan        nan        nan         nan         nan\n",
       "alpha_309     -0.0288    1.3e+06  -2.21e-08      1.000   -2.55e+06    2.55e+06\n",
       "alpha_310      0.0060   3.99e+06    1.5e-09      1.000   -7.82e+06    7.82e+06\n",
       "alpha_311      0.0429        nan        nan        nan         nan         nan\n",
       "alpha_312      0.0898   1.32e+06    6.8e-08      1.000   -2.59e+06    2.59e+06\n",
       "alpha_313     -0.0332        nan        nan        nan         nan         nan\n",
       "alpha_314     -0.0151        nan        nan        nan         nan         nan\n",
       "alpha_315      0.1044        nan        nan        nan         nan         nan\n",
       "alpha_316     -0.2449        nan        nan        nan         nan         nan\n",
       "alpha_317     -0.2731    2.7e+06  -1.01e-07      1.000   -5.29e+06    5.29e+06\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the estimation results\n",
    "kernel_model.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more interesting to compute the confusion matrix and the precision, recall and F-score metric such as in the previous model. Nonetheless, the first step must be to compute the long format kernel matrix for the test set that is necessary to compute the previous metrics using KLR. This can be done using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comute the long format kernel matrix for the test set.\n",
    "K_long_format_test = pkl.long_format_with_kernel_matrix(test_set_scaled, \n",
    "                                                        custom_alt_id, \n",
    "                                                        obs_id_column, \n",
    "                                                        choice_column, \n",
    "                                                        variables, \n",
    "                                                        kernel_type = \"RBF\", \n",
    "                                                        Z = train_set_scaled,\n",
    "                                                        length_scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  2   3   4\n",
       "1  58  0   3  11\n",
       "2   4  0   0   6\n",
       "3   3  0  13   4\n",
       "4   6  0   8  20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the confunsion matrix using the test set\n",
    "confusion_matrix = kernel_model.confusion_matrix(K_long_format_test)\n",
    "\n",
    "# The generated confusion matrix can be shown directly as a pandas dataframe\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Confusion Matrix')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5b35f1748>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the confusion matrix using seaborn.\n",
    "# Generate a new figure and set the figure size.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Generate a heatmap for the confusion matrix using seaborn\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, linewidth=0.5, cmap='PuBu')\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted alternative')\n",
    "ax.set_ylabel('True alternative')\n",
    "ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6691176470588235, 0.6691176470588235, 0.6691176470588235)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the precision, recall and F1-score for the linear model using a\n",
    "# Micro-averaging technique. The F1-score is the F-score for beta = 1.\n",
    "kernel_model.precision_recall_fscore(K_long_format_test, \"micro\", beta = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Prediction using the estimated model\n",
    "\n",
    "Again, once the model has been estimated, new instances can be predicted using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.540218</td>\n",
       "      <td>0.255063</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>0.147908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613830</td>\n",
       "      <td>0.259101</td>\n",
       "      <td>0.028578</td>\n",
       "      <td>0.098491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.951935</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.746670</td>\n",
       "      <td>0.101531</td>\n",
       "      <td>0.137129</td>\n",
       "      <td>0.014670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.121108</td>\n",
       "      <td>0.033822</td>\n",
       "      <td>0.708843</td>\n",
       "      <td>0.136227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.328868</td>\n",
       "      <td>0.121743</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.324588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.505866</td>\n",
       "      <td>0.132691</td>\n",
       "      <td>0.159849</td>\n",
       "      <td>0.201594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.021582</td>\n",
       "      <td>0.031682</td>\n",
       "      <td>0.759434</td>\n",
       "      <td>0.187302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.899178</td>\n",
       "      <td>0.056069</td>\n",
       "      <td>0.029523</td>\n",
       "      <td>0.015229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.519442</td>\n",
       "      <td>0.132519</td>\n",
       "      <td>0.260177</td>\n",
       "      <td>0.087863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "0    0.540218  0.255063  0.056811  0.147908\n",
       "1    0.613830  0.259101  0.028578  0.098491\n",
       "2    0.951935  0.034707  0.004550  0.008808\n",
       "3    0.746670  0.101531  0.137129  0.014670\n",
       "4    0.121108  0.033822  0.708843  0.136227\n",
       "..        ...       ...       ...       ...\n",
       "131  0.328868  0.121743  0.224800  0.324588\n",
       "132  0.505866  0.132691  0.159849  0.201594\n",
       "133  0.021582  0.031682  0.759434  0.187302\n",
       "134  0.899178  0.056069  0.029523  0.015229\n",
       "135  0.519442  0.132519  0.260177  0.087863\n",
       "\n",
       "[136 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The different observations from the test_set are represented on each row.\n",
    "# The columns represent each of the alternatives. The value from each cell\n",
    "# is the probability that the alternative on the corresponding column was\n",
    "# selected on that observation.\n",
    "pd.DataFrame(data=kernel_model.predict(K_long_format_test).reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Computation of the Willingness to Pay (WTP) indicator using the estimated KLR model\n",
    "\n",
    "As shown previously, the equation to compute the WTP is:\n",
    "\\begin{equation}\n",
    "    WTP = \\frac{\\delta_{i n}^{c}}{\\delta_{i n}^{x}}=-\\frac{\\left(\\partial V_{i n} / \\partial x_{i n}\\right)\\left(c_{i n}, x_{i n}\\right)}{\\left(\\partial V_{i n} / \\partial c_{i n}\\right)\\left(c_{i n}, x_{i n}\\right)}\n",
    "\\end{equation}\n",
    "where $c_{in}$ is the cost of the alternative $i$ for a decision-maker $n$ and $x_{in}$ is the value of another variable from the model. The WTP can be computed directly using a close expression for the linear RUM (as shown in section 2.5, nonetheless, for the KLR models it is necessary to compute the gradient vector of the partial derivatives of the systematic utility function. \n",
    "\n",
    "Since KLR is a non-linear model, the first step is to define the point where the WTP is going to be computed. Any observation from a long format dataframe can be selected as this WTP point and, therefore, the WTP for that decision-maker can be computed. Moreover, the analyst can use any point for the WTP calculation, but this point should be provided in a long format dataframe.\n",
    "\n",
    "The following lines of code shows how to compute the WTP for any observation from the dataframe using KLR. The first step is to select the WTP point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_id</th>\n",
       "      <th>mode_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>cost</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.864544</td>\n",
       "      <td>17.530151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568055</td>\n",
       "      <td>26.731061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.167807</td>\n",
       "      <td>55.347093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.978343</td>\n",
       "      <td>32.651246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    custom_id  mode_id  choice      cost       time\n",
       "44         12        1       1  1.864544  17.530151\n",
       "45         12        2       0  0.568055  26.731061\n",
       "46         12        3       0  2.167807  55.347093\n",
       "47         12        4       0  1.978343  32.651246"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the WTP of any observation from the dataframe\n",
    "WTP_point = long_mode_choice.loc[long_mode_choice[obs_id_column] == 12]\n",
    "\n",
    "WTP_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the PyKernelLogit function calculate_WTP_kernel can be used to calculate the WTP indicador for an observation using KLR by numerically approximating the gradient vector of the systematic utility. This function takes following input arguments:\n",
    "* WTP_cost_column. Column name with contains the cost or price of the alternative.\n",
    "\n",
    "* WTP_x_column. Column name with contains the variable to be analyzed.\n",
    "\n",
    "* alt. Indicates the alternative for which the WTP is computed.\n",
    "\n",
    "* model. A MNDC_Model instance corresponding with the trained model.\n",
    "\n",
    "* variables. A Python dictionary where each key represents each of the alternatives IDs and its associated value contains a list of characteristics from the long dataframe WTP_point and Z to be used on that alternative.\n",
    "\n",
    "* WTP_point. A dataframe in long format with only one observation (1 row per each alternative of that observation) for which the Willingness to Pay is going to be computed.\n",
    "\n",
    "* Z. A reference matrix $\\mathbf{Z}_{i}$ to be used when computing the kernel matrix.\n",
    "\n",
    "* kernel_type. The type of kernel function to be used.\n",
    "\n",
    "* scaler. If none scaler was used to fit the kernel model, this parameter must be None. Otherwise, an instance of the scaler function must be provided here. The provided scaler must contain a transform function which scale the features of the dataset according to the scaler definition.\n",
    "\n",
    "* epsilon. The $\\varepsilon$ parameter. The increment of the coordinate vector used for determining the function gradient.  \n",
    "\n",
    "As it was described previously the Value of time (VOT) reflects the price that a traveler is willing to pay to decrease the travel time in one unit. In this example, it is interesting to compute the **VOT** as the model contains the cost and time variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.420462876157751"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comuthe the VOT for a KLR model, i.e. the WTP using cost an time variables.\n",
    "# Take into account that travel time is in minutes, therefore, the VOT must be\n",
    "# multiplied by 60 to convert it to hours.\n",
    "VOT = -60 * pkl.calculate_WTP_kernel(WTP_cost_column = 'cost', \n",
    "                                    WTP_x_column = 'time', \n",
    "                                    alt = 1, \n",
    "                                    model = kernel_model, \n",
    "                                    variables = variables, \n",
    "                                    WTP_point = WTP_point, \n",
    "                                    Z = train_set_scaled, \n",
    "                                    kernel_type = \"RBF\", \n",
    "                                    scaler = scaler)\n",
    "\n",
    "VOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyst can evalute the WTP also for an hypothetical observation which is not present at the dataframe, but this point should be provided in a long format dataframe. In this case, the obs_id_col, alt_id_col and choice_col must be created by the user manually. To illustrate this concept, the following code computes the WTP for an observation for which the value of each variable is the mean value of that variable in the Mode dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_id</th>\n",
       "      <th>mode_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>cost</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.873487</td>\n",
       "      <td>37.044117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.686348</td>\n",
       "      <td>39.771005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.035669</td>\n",
       "      <td>39.923409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.211683</td>\n",
       "      <td>39.504507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   custom_id  mode_id  choice      cost       time\n",
       "0        1.0      1.0     1.0  4.873487  37.044117\n",
       "1        1.0      2.0     0.0  1.686348  39.771005\n",
       "2        1.0      3.0     0.0  2.035669  39.923409\n",
       "3        1.0      4.0     0.0  2.211683  39.504507"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the WTP_points columns. \n",
    "WTP_point_columns = [obs_id_column, custom_alt_id, choice_column, 'cost', 'time']\n",
    "WTP_point = pd.DataFrame(columns=WTP_point_columns)\n",
    "\n",
    "# Append to WTP_points dataframe the mean value for each dataframe variable\n",
    "WTP_point = WTP_point.append({ obs_id_column: 1, # Fixed to 1, necessary to create a long type dataframe\n",
    "                               choice_column: 1, # Fixed to 1 in the first alternative. Necessary to create a long type dataframe\n",
    "                               custom_alt_id: 1, # It indicates the alternative for which the data is stored\n",
    "                               'cost': mode_choice['cost.car'].mean(),\n",
    "                               'time': mode_choice['time.car'].mean(),\n",
    "                               }, ignore_index=True)\n",
    "WTP_point = WTP_point.append({ obs_id_column: 1, # Fixed to 1, necessary to create a long type dataframe\n",
    "                               choice_column: 0, # Fixed to 0 in the rest of alternatives\n",
    "                               custom_alt_id: 2, # It indicates the alternative for which the data is stored\n",
    "                               'cost': mode_choice['cost.carpool'].mean(),\n",
    "                               'time': mode_choice['time.carpool'].mean(),\n",
    "                               }, ignore_index=True)\n",
    "WTP_point = WTP_point.append({ obs_id_column: 1, # Fixed to 1, necessary to create a long type dataframe\n",
    "                               choice_column: 0, # Fixed to 0 in the rest of alternatives\n",
    "                               custom_alt_id: 3, # It indicates the alternative for which the data is stored\n",
    "                               'cost': mode_choice['cost.bus'].mean(),\n",
    "                               'time': mode_choice['time.bus'].mean(),\n",
    "                               }, ignore_index=True)\n",
    "WTP_point = WTP_point.append({ obs_id_column: 1, # Fixed to 1, necessary to create a long type dataframe\n",
    "                               choice_column: 0, # Fixed to 0 in the rest of alternatives\n",
    "                               custom_alt_id: 4, # It indicates the alternative for which the data is stored\n",
    "                               'cost': mode_choice['cost.rail'].mean(),\n",
    "                               'time': mode_choice['time.rail'].mean(),\n",
    "                               }, ignore_index=True)\n",
    "\n",
    "# Show the generated dataframe\n",
    "WTP_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.079644126188795"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comuthe the VOT for a KLR model, i.e. the WTP using cost an time variables.\n",
    "VOT = -60 * pkl.calculate_WTP_kernel(WTP_cost_column = 'cost', \n",
    "                                    WTP_x_column = 'time', \n",
    "                                    alt = 1, \n",
    "                                    model = kernel_model, \n",
    "                                    variables = variables, \n",
    "                                    WTP_point = WTP_point, \n",
    "                                    Z = train_set_scaled, \n",
    "                                    kernel_type = \"RBF\", \n",
    "                                    scaler = scaler)\n",
    "\n",
    "VOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Jupyter notebook\n",
    "\n",
    "This Jupyter notebook was developed by Jos ngel Martn Baos on September 2019 to demonstrate the key functionalities of the PyKernelLogit Python package (https://pypi.org/project/pykernellogit/). This package is based on the PyLogit Python package (https://pypi.org/project/pylogit/) developed by Timothy Brathwaite (Timothy Brathwaite and Joan L. Walker. Asymmetric, closed-form, finite-parameter models of multinomial choice. Journal of Choice Modelling. Volume 29, December 2018.). This notebook is based on the example notebook for the PyLogit package.\n",
    "\n",
    "* Jos ngel Martn Baos (https://joseangelmartinb.github.io/). Faculty of Computer Science. University of Castilla-La Mancha. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
